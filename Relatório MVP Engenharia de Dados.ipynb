{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a38598aa-5b0e-4a5b-8d56-cdcd262cfb2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MVP Engenharia de dados\n",
    "### Aluno: Rodrigo de Sousa Araujo\n",
    "### Matrícula: 4052025001727\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e54bbfb-3b1d-4a92-91a7-cca39b4833ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ODS Objetivos de Desenvolvimento Sustentável\n",
    "\n",
    "UNSDCF Marco de Cooperação das Nações Unidas para o Desenvolvimento Sustentável\n",
    "\n",
    "AFP Agência especializada, fundo e programa\n",
    "\n",
    "RCO Escritório de Coordenação das Nações Unidas\n",
    "\n",
    "UNSDG Portal de Dados do Grupo de Desenvolvimento Sustentável da ONU\n",
    "\n",
    "MEL Monitoramento, Avaliação e Aprendizagem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c4afae6-85e3-48f2-94ec-00365502f8a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Contexto\n",
    "\n",
    "Em 2015, líderes de 193 países adotaram um plano global para combater a fome e a pobreza, bem como para promover o desenvolvimento sustentável para as gerações atuais e futuras, sem deixar ninguém para trás. A partir das lições aprendidas através da implementação dos Objetivos do Milênio (2000-2015), a Agenda das Nações Unidas para o Desenvolvimento Sustentável, aprovada durante a conferência Rio+20 e adotada através da Resolução A/RES/70/1 da Assembleia Geral das Nações Unidas, é um plano de ação que visa a fortalecer a paz universal em maior liberdade, erradicar a pobreza em todas as suas formas e dimensões, incluindo a pobreza extrema, e a tomar as medidas ousadas e transformadoras que são urgentemente necessárias para colocar o mundo em um caminho sustentável e resiliente. \n",
    "\n",
    "A Agenda apresenta 17 Objetivos de Desenvolvimento Sustentável (ODS) e 169 metas integrais e individisíveis:\n",
    "\n",
    "> _Figura 1: Objetivos de Desenvolvimento Sustentável_\n",
    "![](/Workspace/Users/rsousa.rodrigo@gmail.com/mvp-engenharia-de-dados/Figura 1 - ODS)\n",
    "\n",
    "O processo de implementação dos Objetivos, as responsabilidades da Agenda 2030 são globais e compartilhadas por governos, empresas, sociedade civil e indivíduos. A nível programático, a resolução 72/279 da Assembleia Geral eleva o Marco de Cooperação das Nações Unidas para o Desenvolvimento Sustentável (UNSDCF) como “o instrumento mais importante para o planejamento e a implementação das atividades de desenvolvimento da ONU em nível nacional, em apoio à implementação da Agenda 2030 para o Desenvolvimento Sustentável (Agenda 2030)”. Adotado entre o Sistema das Nações Unidas em um determinado país e seu Estado-membro, o Marco de Cooperação orienta todo o ciclo programático de atuação das agências, fundos e programas da ONU em um determinado país para impulsionar o planejamento, a implementação, o monitoramento, a prestação de contas e a avaliação do apoio coletivo da ONU para alcançar a Agenda 2030. O UNSDCF também apoia a configuração do fluxo de recursos e financiamento direcionados ao desenvolvimento sustentável, bem como o estabelecimento de parcerias estratégicas e efetivas.\n",
    "\n",
    "No Brasil, o O atual [**Marco de Cooperação 2023-2027**](https://brasil.un.org/pt-br/274971-marco-de-coopera%C3%A7%C3%A3o-das-na%C3%A7%C3%B5es-unidas-para-o-desenvolvimento-sustent%C3%A1vel-2023-2027) foi assinado pelo Governo Brasileiro e as Nações Unidas no dia 1º de agosto de 2023, no âmbito da visita da vice-secretária-geral da ONU, Amina J. Mohammed, ao país. Para o ciclo programático 2023-2027, 21 agências especializadas, fundos e programas (AFPs) têm estado diretamente envolvidos na implementação e monitoramento do Marco, que apresenta 5 eixos temáticos de atuação:\n",
    "\n",
    "- Transformação econômica para o desenvolvimento sustentável\n",
    "- Inclusão social para o desenvolvimento sustentável\n",
    "- Meio ambiente e mudança do clima para o desenvolvimento sustentável\n",
    "- Governança e capacidades institucionais\n",
    "- Relação das ações humanitárias e de desenvolvimento sustentável\n",
    "\n",
    "> _Figura 2: Resultados esperados do Marco de Cooperação das Nações Unidas para o Desenvolvimento Sustentável 2023-2027 aprovado entre o Governo do Brasil e as Nações Unidas. Fonte: Escritório de Coordenação das Nações Unidas (RCO) no Brasil._\n",
    "![](/Workspace/Users/rsousa.rodrigo@gmail.com/mvp-engenharia-de-dados/Figura 2 - Resultados esperados UNSDCF.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e27df91f-6a31-48f7-bb01-19ce92e32eb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Objeto de análise\n",
    "\n",
    "Em 2023, foi lançada a versão **beta** do **Portal de Dados do Grupo de Desenvolvimento Sustentável da ONU (UNSDG)**. A base de dados apresenta informações sobre o trabalho das equipes da ONU para promover os Objetivos de Desenvolvimento Sustentável em 162 países e territórios. Os dados são extraídos do _UN.INFO_, uma plataforma digital usada pelas equipes nacionais da ONU para aumentar a transparência e a responsabilidade pela coordenação do desenvolvimento.\n",
    "\n",
    "Este projeto tem, portanto, o objetivo de analisar os dados referentes ao Brasil disponibilizados no UNSDCG para responder às seguintes perguntas:\n",
    "1) O financiamento recebido pelas agências, fundos e programas da ONU no Brasil tem sido direcionado a quais ODS?\n",
    "2) Quais são os grupos populacionais mais beneficiados pelo trabalho da ONU em 2025? E os menos beneficiados?\n",
    "3) Qual a natureza do trabalho desenvolvimento pela ONU no Brasil?\n",
    "4) Em uma série histórica (2023 a 2025), **COMPLETAR**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e33a0a42-9409-4eb3-85b1-6a52f75b4fcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Governança dos dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd1dfaa8-606d-416f-9d94-a6f61720e8fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4. Preparação\n",
    "Nesta etapa, farei a preparação do ambiente para que possa fazer o armazenamento do dado em suas diversas versões tratadas. Utilizarei a linguagem SQL para criar os catálogos e esquemas necessários para este exercício:\n",
    "- Catálogo MVP\n",
    "- Esquema Bronze\n",
    "- Esquema Silver\n",
    "- Esquema Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d30ac197-201b-4fec-9190-294b526d93d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 2
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS mvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be542f08-7f54-4f00-9f27-f552c2aa6a47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 3
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "USE CATALOG mvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48e41cb2-2069-4f3a-a431-ef3f1706327a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 4
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea797906-778c-4e89-80db-965f7c17ada6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 5
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS bronze;\n",
    "CREATE SCHEMA IF NOT EXISTS silver;\n",
    "CREATE SCHEMA IF NOT EXISTS gold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08a93d95-817a-4420-9455-778b348290e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 5. Extração dos dados\n",
    "Como mencionado anteriormente, a base de dados gerada a partir do UN.INFO é pública e está disponibilizada no [Portal de Dados do Grupo de Desenvolvimento Sustentável da ONU (UNSDG)](https://uninfo.org/data-explorer/cooperation-framework), na aba \"Data Explorer\".  A base de dados é disponibilizada em formato **.csv**. Os dados, em geral, são atualizados na plataforma trimestralmente, ou a depender da dinâmica de governança de dados do Grupo de Monitoramento, Avaliação e Aprendizagem (MEL) da ONU em cada país.\n",
    "\n",
    "Na linguagem de MEL das Nações Unidas e de acordo com a teoria de mudança do Marco de Cooperação, _outcomes_ correspondem aos resultados esperados no âmbito do Marco de Cooperação, conforme apresentado na seção 2. Por sua vez, _outputs_ correspondem aos diferentes conjuntos de intervenções (ou produtos) através das quais o Sistema ONU agrega valor e que poderiam ajudar o país a superar os principais desafios de desenvolvimento identificados, acelerando o seu progresso na realização dos ODS. Por fim, um _sub-output_ ao nível mais operacional da teoria da mudança, representando ações individuais ou coletivas das agências, fundos e programas que, agregadas, compõem o conjunto de outputs.\n",
    "\n",
    "O Portal de Dados permite a extração de dados em duas camadas de análise: a nível de relatórios de sub-outputs e a nível de indicadores. Para fins deste exercício, será analisada a **base de dados de sub-outputs** por ser a camada mais granular possível dos dados, trazendo informações financeiras e orçamentárias das ações de cada AFP, bem como informações desagregadas sobre beneficiários, natureza das ações e, mais importante para este exercício, os ODS relacionados.\n",
    "\n",
    "O processo de exportação é simples, bastando apenas selecionar, na aba \"Data Explorer\", o país e o plano de desenvolvimento de análise: neste caso, o Marco de Cooperação das Nações Unidas para o Desenvolvimento Sustentável.\n",
    "\n",
    "> Figura 3 - Exportação dos dados a partir do Portal de Dados do UNSDG.\n",
    "> ![](/Workspace/Users/rsousa.rodrigo@gmail.com/mvp-engenharia-de-dados/Figura 3 - Exportação de dados do Portal.png)\n",
    "\n",
    "Um versão original da base de dados foi salva o esquema \"default\", apenas para fins de registro. Como o Portal de dados ainda está em versão beta, o banco de dados apresenta uma série de inconsistências (valores nulos, entradas equivocadas, erros de informação) que serão refinados a seguir, a partir da aplicação da **Arquitetura Medalhão**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d37ec5b5-2a06-42e0-8703-133a68bf8f48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 6. Tratamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad7d0c04-3e59-47ba-b3d2-c5071a56a466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Para fazer a primeira camada de transformação de dados, foi necessário criar um Volume dentro do esquema Default e inserir a base de dados a ser utilizada. Optei por manter, nesse momento, o nome original do arquivo (\"export-2025-12-13.csv).\n",
    "\n",
    "> Figura 4 - Upload da base de dados dentro de volume específico no esquema DEFAULT\n",
    "\n",
    "> ![](/Workspace/Users/rsousa.rodrigo@gmail.com/mvp-engenharia-de-dados/Figura 4 - Upload da base de dados dentro de volume específico no esquema DEFAULT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bafb77a-9aa2-43ca-8db9-4d00a81682e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6.1 Refinamento BRONZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73207a13-0828-4d1b-85ec-50eae3f48dab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Uma vez que a base de dados original possui 65 atributos, optei por já excluir neste primeiro momento todas as colunas que não usarei nesta atividade. Optei por essa exclusão preliminar para facilitar a criação do dicionário de dados, que virá a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c247ee02-8855-40d9-8e41-3b19e8c89e8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Definir o uso padrão do ESQUEMA BRONZE\n",
    "spark.sql(\"USE CATALOG mvp\")\n",
    "spark.sql(\"USE SCHEMA bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3d5d5c3-9d37-4f4f-afd0-49694a3b2b9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Country</th><th>Plan name</th><th>Plan start date</th><th>Plan end date</th><th>Plan ID</th><th>Strategic priority code</th><th>Strategic priority</th><th>Outcome code</th><th>Outcome</th><th>Output code</th><th>Output</th><th>Suboutput also contributes to the following outputs</th><th>Sub-Output publication status</th><th>Sub-Output code</th><th>Sub-Output</th><th>ID</th><th>Description</th><th>Start date</th><th>End date</th><th>Status</th><th>Joint Programmes / Initiatives names</th><th>Joint Programmes / Initiatives types</th><th>Agency abbreviations</th><th>Agencies</th><th>Contributing partners</th><th>Implementation partners</th><th>SDG Targets</th><th>SDG Goals</th><th>Geography</th><th>QCPR function</th><th>Humanitarian marker</th><th>Humanitarian marker narrative</th><th>Gender marker</th><th>Gender marker narrative</th><th>Human rights marker</th><th>Human rights marker narrative</th><th>Sustaining peace marker</th><th>Sustaining peace marker narrative</th><th>LNOB groups targeted</th><th>Focal users</th><th>Non Monetary Assistance</th><th>Total required resources</th><th>Total available resources</th><th>Total expenditure resources</th><th>2023 Required</th><th>2023 Available</th><th>2023 Expenditure</th><th>2023 Narrative</th><th>2024 Required</th><th>2024 Available</th><th>2024 Expenditure</th><th>2024 Narrative</th><th>2025 Required</th><th>2025 Available</th><th>2025 Expenditure</th><th>2025 Narrative</th><th>2026 Required</th><th>2026 Available</th><th>2026 Expenditure</th><th>2026 Narrative</th><th>2027 Required</th><th>2027 Available</th><th>2027 Expenditure</th><th>2027 Narrative</th><th>Brazil - Other beneficiaries</th></tr></thead><tbody><tr><td>Brazil</td><td>United Nations Sustainable Development Cooperation Framework</td><td>2023-01-01</td><td>2027-12-31</td><td>1510</td><td>5</td><td>5. Relação das Ações Humanitárias e de Desenvolvimento Sustentável</td><td>1</td><td>5.1 Até 2027, o Brasil terá suas estratégias, políticas públicas e capacidades institucionais fortalecidas e ampliadas, em todos os níveis de governo e em articulação com o setor privado e a sociedade civil, para prevenir, mitigar e responder a crises humanitárias e desastres, com base em evidências, com especial atenção às populações afetadas, às pessoas refugiadas, migrantes e apátridas e demais grupos e pessoas em situação de vulnerabilidade, em sua diversidade, combatendo a xenofobia e a intolerância e promovendo o respeito aos direitos humanos, a igualdade de geração, gênero, raça e etnia e o desenvolvimento sustentável.</td><td>3</td><td>5.1.3 Capacidades fortalecidas de grupos e pessoas em situação de vulnerabilidade, a partir da perspectiva de geração, gênero, raça e etnia, para participar efetivamente de diálogos e formação de consensos sobre políticas, planos, leis e orçamentos, em especial sobre ações de prevenção, mitigação, assistência e resposta a crises humanitárias e desastres e fluxos de pessoas refugiadas, migrantes e apátridas, por meio de mecanismos formulados e implementados em todos os níveis de governo.</td><td>-</td><td>Yes</td><td>01</td><td>UNHCR Increased community self-management and capacity of forcibly displaced and stateless persons through age, gender and diversity approach to support integration and resilience.</td><td>134839</td><td>Training in community-self management to forcibly displaced and stateless persons on how to address protection and integration issues in Brazil; identification and proposition of solutions considering intersectionality and the impacts of climate change.</td><td>2023-01-01</td><td>2025-12-31</td><td>Implementation</td><td>-</td><td>-</td><td>UNHCR</td><td>United Nations High Commissioner for Refugees</td><td>#BR Gerdau; European Commission; Inmigration, Refugees and Citizenship Canada; Japan; The US Government Department of State's Bureau of Population, Refugees and Migration; United Nations High Commissioner for Refugees</td><td>#BR Associação Voluntários para o Serviço Internacional; #BR Cáritas - Arquidiocesana do Rio de Janeiro; #BR Cáritas - Arquiodiocesana de São Paulo; #BR Cáritas Brasileira - Regional Paraná; #BR Instituto Internacional de Educação do Brasil; #BR Instituto Mana; #BR Organizacao Fraternidade sem Fronteiras; #BR Serviço Jesuíta a Migrantes e Refugiados</td><td>5.1 End all forms of discrimination against all women and girls everywhere.,5.2 Eliminate all forms of violence against all women and girls in the public and private spheres, including trafficking and sexual and other types of exploitation.,10.2 By 2030, empower and promote the social, economic and political inclusion of all, irrespective of age, sex, disability, race, ethnicity, origin, religion or economic or other status.,10.3 Ensure equal opportunity and reduce inequalities of outcome, including by eliminating discriminatory laws, policies and practices and promoting appropriate legislation, policies and action in this regard.,16.2 End abuse, exploitations, trafficking and all forms of violence against and torture of children.,16.3 Promote the rule of law at the national and international levels and ensure equal access to justice for all.,16.9 By 2030, provide legal identity for all, including birth registration.,16.b Promote and enforce non-discriminatory laws and policies for sustainable development.,17.3 Mobilize additional financial resources for developing countries from multiple sources.,17.9 Enhance international support for implementing effective and targeted capacity-building in developing countries to support national plans to implement all the sustainable development goals, including through North-South, South-South and triangular cooperation.</td><td>5 Gender Equality; 10 Reduced Inequalities; 16 Peace and Justice - Strong Institutions; 17 Partnerships for the Goals</td><td>Level 0: Brazil (Level 1: Paraná (PR); Rio Grande do Sul (RS); São Paulo (SP); Rio de Janeiro (RJ); Roraima (RR); Pará (PA); Amazonas (AM); Minas Gerais (MG); Distrito Federal (DF); Santa Catarina (SC))</td><td>Direct Support/ Service Delivery; Policy Advice and Thought Leadership; Other (including coordination); Capacity Development/Technical Assistance; Normative Support</td><td>-</td><td>-</td><td>2 - Gender equality/women's empowerment is a significant objective</td><td>The trainings on this sub-output aim to empower forcibly displaced and stateless persons through community-self management. The goal is to equip participants with the knowledge and tools to navigate protection and integration issues, fostering an environment where all members, regardless of gender, can participate and contribute to solutions that address their unique circumstances and needs.</td><td>1 - Limited contribution to realization of human rights</td><td>null</td><td>1 - Contributes to sustaining peace empowerment in a limited way</td><td>null</td><td>Refugees & Asylum Seekers; Stateless Persons</td><td>Italo Ribeiro Alves</td><td>null</td><td>2192245</td><td>957493</td><td>916345</td><td>730749</td><td>532569</td><td>532569</td><td>\"With UNHCR's support, refugee leaders advocated for their needs in decision-making arenas, including local councils, congressional hearings, national policy consultations, and the Global Refugee Forum. Indigenous leaders participated in key forums like the Amazon Summit and \"\"Acampamento Terra Livre</td><td>\"\" advancing demands with federal authorities. UNHCR facilitated government recognition of indigenous refugees and supported the creation of a multi-sectoral group for indigenous refugee policies.UNHCR collaborated with 45 refugee-led organizations</td><td> providing small grants to 26</td><td> including 9 women-led</td><td> for projects promoting social cohesion. Training was offered to enhance protection</td><td> resource mobilization</td><td> programming</td><td> and accountability. Women-led organizations were also mobilized in gender-based violence prevention efforts across various states.Refugees gained better access to information on their rights and services through digital communication</td><td> traditional outreach</td><td> and tools like a hotline</td><td> Chatbot</td><td> and the Help website</td><td> which reached over 380</td><td>000 users. Additionally</td><td> 531 vulnerable refugees benefited from the Federal Government's relocation strategy</td><td> receiving personalized assistance and support in navigating public services.\"</td><td>730748</td><td>219188</td></tr><tr><td>Brazil</td><td>United Nations Sustainable Development Cooperation Framework</td><td>2023-01-01</td><td>2027-12-31</td><td>1510</td><td>1</td><td>1. Transformação Econômica para o Desenvolvimento Sustentável</td><td>2</td><td>1.2 Em 2027, o Brasil terá avançado na inclusão econômica que contribui para a redução da pobreza, da fome, das vulnerabilidades, das desigualdades, e da discriminação de geração, gênero, raça e etnia, e que garante o direito à educação transformadora para o pleno desenvolvimento da pessoa e o acesso ao trabalho decente, às oportunidades para geração de renda, à proteção social, econômica e políticas de cuidados e às infraestruturas resilientes, assegurando a igualdade de oportunidades e a sua sustentabilidade.</td><td>4</td><td>1.2.4 Povos indígenas, povos e comunidades tradicionais e populações do campo, floresta e águas em geral com capacidades fortalecidas para acessar alternativas inovadoras de renda, da preservação e aproveitamento de tradições locais e saberes geracionais para um uso sustentável da sociobiodiversidade, com promoção de produtos locais, da agroecologia, dos sistemas alimentares, do manejo sustentável dos recursos naturais e da pesca sustentável.</td><td>-</td><td>Yes</td><td>1</td><td>FAO UTF/BRA/084/BRA Aquaculture Sustainably Developed</td><td>134270</td><td>Contribute to the sustainable development of Aquaculture by promoting the strengthening of the aquaculture production chain, its diversity and ruling to generate increased production and social inclusion and provide increased income and job offers for low-income populations and other relevant players in the sector.\n",
       "</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>Immediate Objective 1: Improve the performance of the aquaculture productive chains.</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>Immediate Objective 2: Set sustainability criteria to the aquaculture parks.</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>Immediate Objective 3: Control quality in the production of water animals and aquaculture products.</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>Immediate Objective 4: Structure monitoring and analysis tools to the National Plan for Aquaculture Monitoring- PNMA.</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>Immediate Objective 5: Restructure the legal framework of the Brazilian aquaculture legislation.</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>Immediate Objective 6: Expand the institutional sustainability of the sector through studies</td><td> planning and execution of management actions.\"</td><td>2023-01-02</td><td>2023-12-31</td><td>Closed</td><td>-</td><td>-</td><td>FAO</td><td>Food and Agriculture Organization of the United Nations</td><td>Brazilian Ministry of Fishing and Aquaculture</td><td>Brazilian Ministry of Fishing and Aquaculture</td><td>12.2 By 2030, achieve the sustainable management and efficient use of natural resources.</td><td>12 Responsible Consumption and Production</td><td>Level 0: Brazil</td><td>Capacity Development/Technical Assistance</td><td>-</td><td>-</td><td>0 - Not expected to contribute to gender equality/women's empowerment</td><td>According to its original design, the Project aimed, in general, to benefit the country's aquaculture sector, through social inclusion initiatives and employment and income generation opportunities related to the aquaculture production chain. As for the potential beneficiaries, they are made up of Brazilian society, governmental and non-governmental organizations, private companies, associations, aquaculture communities, riverside communities, quilombolas, community associations, research centers, extensionists, and professionals working in the sector, and other relevant partners. Due to the discontinuity of the aquaculture parks program, the Project fell short in stimulating occupation and consolidating production in the demarcated aquaculture parks, thus not being able to fulfill its aspiration of including the diversity of the sector, especially about the most vulnerable sectors and social inclusion and also did not adequately incorporate a gender perspective.</td><td>1 - Limited contribution to realization of human rights</td><td>null</td><td>0 - Not expected to contribute towards sustaining peace</td><td>null</td><td>Peasants & Rural Workers</td><td>Sue Takei; Sergio Dorfler</td><td>null</td><td>8349148</td><td>5828949</td><td>5747390</td><td>8349148</td><td>5828949</td><td>5747390</td><td>Projects activities closed in 2023. The main results of UTF/BRA/084/BRA were surveys/diagnoses of aquaculture productive systems, continental and marine, besides the evaluation of the effectiveness of public programs/policies and improvement of technical and managerial tools to support public activity. The project made important contributions to the Ministry of Fisheries and Aquaculture, mainly focused on planning and development of production chains, aquaculture in Union waters, environmental licensing, and monitoring and management mechanisms, which have certainly brought considerable sectoral gains to increased production and social inclusion intended by the Project, and which will certainly continue to contribute to the expected success of the activity in the coming years. Even without mechanisms that establish a direct correlation between the execution of the Project and the national aquaculture production, there was a considerable increase in national aquaculture production over the duration of the Project: a growth of 36%, from 476,521 to 648,537 tons, respectively – an average of 4% annual growth. Regarding the values generated in primary production, there was a growth of 126% in the same period (annual average of 11% growth), reaching R$ 6.9 billion in 2021 (PPM-IBGE, 2022). Therefore, the Project favored incremental changes that contributed to greater qualification and scope of public policies that benefited national aquaculture, strengthening a base of evidence, analyses, and propositions that allowed conceptual and methodological advances in policies that constitute a legacy to be potentially resumed and deepened. One achievement to be highlighted in 2023 was the Aquaculture Multipliers course, which contributes to improve the quality and effectiveness of public services related to the development of aquaculture, by carrying out a training course aimed at municipal and state public agents working in aquaculture, which covers a wide range of aquaculture-related subjects, providing participants with essential knowledge to become multipliers and promoters of this activity. The course was launched on 13 July 2023 by the Ministry of Fisheries and Aquaculture. This is a partnership between the Ministry, the universities of Brasília (UnB) and Rio Grande do Norte (UFRN) and FAO. It consists of the distribution of essential knowledge about aquaculture, over 160 hours of classes divided into modules. The course is completely free and 100% online. Currently, it has more than three thousand subscribers from Brazil and abroad. The link to the platform is: Plataforma - Multiplicadores Aquicolas (mpa.gov.br).</td><td>null</td><td>null</td><td>null</td><td>-</td><td>null</td><td>null</td><td>null</td><td>-</td><td>null</td><td>null</td><td>null</td><td>-</td><td>null</td><td>null</td><td>null</td><td>-</td><td>General Population; Quilombola, ribeirinhos and other traditional popualtions</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>Brazil</td><td>United Nations Sustainable Development Cooperation Framework</td><td>2023-01-01</td><td>2027-12-31</td><td>1510</td><td>1</td><td>1. Transformação Econômica para o Desenvolvimento Sustentável</td><td>2</td><td>1.2 Em 2027, o Brasil terá avançado na inclusão econômica que contribui para a redução da pobreza, da fome, das vulnerabilidades, das desigualdades, e da discriminação de geração, gênero, raça e etnia, e que garante o direito à educação transformadora para o pleno desenvolvimento da pessoa e o acesso ao trabalho decente, às oportunidades para geração de renda, à proteção social, econômica e políticas de cuidados e às infraestruturas resilientes, assegurando a igualdade de oportunidades e a sua sustentabilidade.</td><td>1</td><td>1.2.1 Governos nacionais e subnacionais, instituições públicas e setor privado têm suas capacidades reforçadas para formular e implementar políticas e iniciativas para promoção do desenvolvimento econômico local, da geração de renda, da ampliação da empregabilidade e garantia do trabalho decente, com foco em igualdade de oportunidades, na redução da discriminação nas relações econômicas e trabalhistas, das diferenças salariais por sexo, geração, gênero, raça, etnia ou nacionalidade, no fortalecimento dos direitos de trabalhadores, dos usuários e das comunidades afetadas pela atividade econômica, e na promoção da inclusão em condições igualitárias de mulheres, jovens, pessoas com deficiência, migrantes e pessoas refugiadas, de modo a proteger os direitos dos segmentos da população em situação de vulnerabilidade.</td><td>-</td><td>Yes</td><td>1</td><td>IOM IS.0208 Enhancing Protection, Regularization, and Socio-economic Integration of Migrants and Refugees from Venezuela and Other Relevant Nationalities in the Latin America Region</td><td>200514</td><td>This project aims to improve access to regularization and documentation processes for vulnerable migrants, strengthen protection mechanisms, and foster long-term integration of migrants into host communities in Acre and Rondônia, in northern Brazil. Particular attention will be given to border towns, such as Assis Brasil, (Acre) and Guajará-Mirim (Rondônia), where protection needs are heightened due to increased patterns of irregular migration and multiple coexisting migration routes.</td><td>2025-02-07</td><td>2027-02-06</td><td>Implementation</td><td>-</td><td>-</td><td>IOM</td><td>International Organization for Migration</td><td>Global Affairs Canada</td><td>#BR Federal Police of Brazil; Brazilian Ministry of Development, Social Assistance, Family and Fight Against Hunger; Brazilian Ministry of Justice and Public Security; Brazilian Public Defenders' Office</td><td>5.1 End all forms of discrimination against all women and girls everywhere.,5.2 Eliminate all forms of violence against all women and girls in the public and private spheres, including trafficking and sexual and other types of exploitation.,10.3 Ensure equal opportunity and reduce inequalities of outcome, including by eliminating discriminatory laws, policies and practices and promoting appropriate legislation, policies and action in this regard.,10.7 Facilitate orderly, safe, regular and responsible migration and mobility of people, including through the implementation of planned and well-managed migration policies.,16.2 End abuse, exploitations, trafficking and all forms of violence against and torture of children.</td><td>5 Gender Equality; 10 Reduced Inequalities; 16 Peace and Justice - Strong Institutions</td><td>Level 0: Brazil (Level 1: Rondônia (RO); Acre (AC))</td><td>Direct Support/ Service Delivery; Capacity Development/Technical Assistance</td><td>-</td><td>-</td><td>2 - Gender equality/women's empowerment is a significant objective</td><td>null</td><td>3 - Principal contribution is to the realization of human rights</td><td>null</td><td>0 - Not expected to contribute towards sustaining peace</td><td>null</td><td>-</td><td>Fernanda Garcia</td><td>null</td><td>558267</td><td>558267</td><td>12813</td><td>null</td><td>null</td><td>null</td><td>-</td><td>null</td><td>null</td><td>null</td><td>-</td><td>232859</td><td>232859</td><td>12813</td><td>-</td><td>278921</td><td>278921</td><td>null</td><td>-</td><td>46487</td><td>46487</td><td>null</td><td>-</td><td>-</td></tr><tr><td>Brazil</td><td>United Nations Sustainable Development Cooperation Framework</td><td>2023-01-01</td><td>2027-12-31</td><td>1510</td><td>4</td><td>4. Governança e Capacidades Institucionais</td><td>1</td><td>4.1 Em 2027, o Brasil terá fortalecido, no marco do estado democrático de direito, sua governança, legislação, capacidades e articulação institucionais com ampliação da participação popular para elaborar e executar políticas públicas baseadas em evidências, em direitos humanos e igualdade de geração, gênero, raça e etnia, com vistas à prevenção e ao enfrentamento à corrupção, ao crime e às múltiplas formas de violência, e orientadas às especificidades do território e à transparência, com inovação, cooperação nacional e internacional, e ampla participação da sociedade.</td><td>5</td><td>4.1.5 Capacidades governamentais fortalecidas para garantir uma abordagem multidimensional para combater crimes transnacionais, em especial durante crises humanitárias, e mitigar seus impactos sobre grupos e pessoas em situação de vulnerabilidade, compartilhando informações e aprimorando políticas orientadas por evidências, voltadas ao respeito aos direitos humanos e baseadas na igualdade de geração, gênero, raça e etnia.</td><td>-</td><td>Yes</td><td>1</td><td>IOM PX.0650 Improving Reintegration Outcomes for Survivors of Modern Slavery (Phase 2)</td><td>200515</td><td>This project seeks to address these gaps by enhancing the capacities of key actors to improve voluntary return and reintegration processes and outcomes for survivors of modern slavery and human trafficking. Building on activities conducted in the first phase of the project (PX.0543), it will strengthen the understanding and capacity of key actors in the UK and in countries of return, such as Brazil. This will be done by assessing the role of police and local authorities in the voluntary return of survivors from the UK, as well as by building the capacity of key actors involved in voluntary return and reintegration and conducting or updating national mappings of available reintegration services in countries of return. Second, the project will enhance the engagement of survivors of modern slavery and human trafficking in the process of voluntary return and reintegration, to centre survivor voice and agency, by conducting lived experience research to understand the experiences of survivors who have returned, and by piloting a Virtual Orientation programme for survivors considering return from the UK, promoting survivor agency in decision making.</td><td>2025-07-01</td><td>2026-03-31</td><td>Implementation</td><td>-</td><td>-</td><td>IOM</td><td>International Organization for Migration</td><td>UK Home Office</td><td>UK Home Office</td><td>5.1 End all forms of discrimination against all women and girls everywhere.,5.2 Eliminate all forms of violence against all women and girls in the public and private spheres, including trafficking and sexual and other types of exploitation.,8.7 Take immediate and effective measures to eradicate forced labour, end modern slavery and human trafficking and secure the prohibition and elimination of the worst forms of child labour, including recruitment and use of child soldiers, and by 2025 end child labour in all its forms.,10.7 Facilitate orderly, safe, regular and responsible migration and mobility of people, including through the implementation of planned and well-managed migration policies.</td><td>5 Gender Equality; 8 Decent Jobs and Economic Growth; 10 Reduced Inequalities</td><td>Level 0: Brazil</td><td>Capacity Development/Technical Assistance; Direct Support/ Service Delivery</td><td>-</td><td>-</td><td>2 - Gender equality/women's empowerment is a significant objective</td><td>null</td><td>3 - Principal contribution is to the realization of human rights</td><td>null</td><td>0 - Not expected to contribute towards sustaining peace</td><td>null</td><td>Migrants; Victims of grave human rights violations of (slavery, torture, trafficking, sexual exploitation and abuse...)</td><td>Fernanda Garcia</td><td>null</td><td>106715</td><td>106715</td><td>14212</td><td>null</td><td>null</td><td>null</td><td>-</td><td>null</td><td>null</td><td>null</td><td>-</td><td>77191</td><td>77191</td><td>14212</td><td>-</td><td>29524</td><td>29524</td><td>null</td><td>-</td><td>null</td><td>null</td><td>null</td><td>-</td><td>-</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Brazil",
         "United Nations Sustainable Development Cooperation Framework",
         "2023-01-01",
         "2027-12-31",
         "1510",
         "5",
         "5. Relação das Ações Humanitárias e de Desenvolvimento Sustentável",
         "1",
         "5.1 Até 2027, o Brasil terá suas estratégias, políticas públicas e capacidades institucionais fortalecidas e ampliadas, em todos os níveis de governo e em articulação com o setor privado e a sociedade civil, para prevenir, mitigar e responder a crises humanitárias e desastres, com base em evidências, com especial atenção às populações afetadas, às pessoas refugiadas, migrantes e apátridas e demais grupos e pessoas em situação de vulnerabilidade, em sua diversidade, combatendo a xenofobia e a intolerância e promovendo o respeito aos direitos humanos, a igualdade de geração, gênero, raça e etnia e o desenvolvimento sustentável.",
         "3",
         "5.1.3 Capacidades fortalecidas de grupos e pessoas em situação de vulnerabilidade, a partir da perspectiva de geração, gênero, raça e etnia, para participar efetivamente de diálogos e formação de consensos sobre políticas, planos, leis e orçamentos, em especial sobre ações de prevenção, mitigação, assistência e resposta a crises humanitárias e desastres e fluxos de pessoas refugiadas, migrantes e apátridas, por meio de mecanismos formulados e implementados em todos os níveis de governo.",
         "-",
         "Yes",
         "01",
         "UNHCR Increased community self-management and capacity of forcibly displaced and stateless persons through age, gender and diversity approach to support integration and resilience.",
         "134839",
         "Training in community-self management to forcibly displaced and stateless persons on how to address protection and integration issues in Brazil; identification and proposition of solutions considering intersectionality and the impacts of climate change.",
         "2023-01-01",
         "2025-12-31",
         "Implementation",
         "-",
         "-",
         "UNHCR",
         "United Nations High Commissioner for Refugees",
         "#BR Gerdau; European Commission; Inmigration, Refugees and Citizenship Canada; Japan; The US Government Department of State's Bureau of Population, Refugees and Migration; United Nations High Commissioner for Refugees",
         "#BR Associação Voluntários para o Serviço Internacional; #BR Cáritas - Arquidiocesana do Rio de Janeiro; #BR Cáritas - Arquiodiocesana de São Paulo; #BR Cáritas Brasileira - Regional Paraná; #BR Instituto Internacional de Educação do Brasil; #BR Instituto Mana; #BR Organizacao Fraternidade sem Fronteiras; #BR Serviço Jesuíta a Migrantes e Refugiados",
         "5.1 End all forms of discrimination against all women and girls everywhere.,5.2 Eliminate all forms of violence against all women and girls in the public and private spheres, including trafficking and sexual and other types of exploitation.,10.2 By 2030, empower and promote the social, economic and political inclusion of all, irrespective of age, sex, disability, race, ethnicity, origin, religion or economic or other status.,10.3 Ensure equal opportunity and reduce inequalities of outcome, including by eliminating discriminatory laws, policies and practices and promoting appropriate legislation, policies and action in this regard.,16.2 End abuse, exploitations, trafficking and all forms of violence against and torture of children.,16.3 Promote the rule of law at the national and international levels and ensure equal access to justice for all.,16.9 By 2030, provide legal identity for all, including birth registration.,16.b Promote and enforce non-discriminatory laws and policies for sustainable development.,17.3 Mobilize additional financial resources for developing countries from multiple sources.,17.9 Enhance international support for implementing effective and targeted capacity-building in developing countries to support national plans to implement all the sustainable development goals, including through North-South, South-South and triangular cooperation.",
         "5 Gender Equality; 10 Reduced Inequalities; 16 Peace and Justice - Strong Institutions; 17 Partnerships for the Goals",
         "Level 0: Brazil (Level 1: Paraná (PR); Rio Grande do Sul (RS); São Paulo (SP); Rio de Janeiro (RJ); Roraima (RR); Pará (PA); Amazonas (AM); Minas Gerais (MG); Distrito Federal (DF); Santa Catarina (SC))",
         "Direct Support/ Service Delivery; Policy Advice and Thought Leadership; Other (including coordination); Capacity Development/Technical Assistance; Normative Support",
         "-",
         "-",
         "2 - Gender equality/women's empowerment is a significant objective",
         "The trainings on this sub-output aim to empower forcibly displaced and stateless persons through community-self management. The goal is to equip participants with the knowledge and tools to navigate protection and integration issues, fostering an environment where all members, regardless of gender, can participate and contribute to solutions that address their unique circumstances and needs.",
         "1 - Limited contribution to realization of human rights",
         null,
         "1 - Contributes to sustaining peace empowerment in a limited way",
         null,
         "Refugees & Asylum Seekers; Stateless Persons",
         "Italo Ribeiro Alves",
         null,
         "2192245",
         "957493",
         "916345",
         "730749",
         "532569",
         "532569",
         "\"With UNHCR's support, refugee leaders advocated for their needs in decision-making arenas, including local councils, congressional hearings, national policy consultations, and the Global Refugee Forum. Indigenous leaders participated in key forums like the Amazon Summit and \"\"Acampamento Terra Livre",
         "\"\" advancing demands with federal authorities. UNHCR facilitated government recognition of indigenous refugees and supported the creation of a multi-sectoral group for indigenous refugee policies.UNHCR collaborated with 45 refugee-led organizations",
         " providing small grants to 26",
         " including 9 women-led",
         " for projects promoting social cohesion. Training was offered to enhance protection",
         " resource mobilization",
         " programming",
         " and accountability. Women-led organizations were also mobilized in gender-based violence prevention efforts across various states.Refugees gained better access to information on their rights and services through digital communication",
         " traditional outreach",
         " and tools like a hotline",
         " Chatbot",
         " and the Help website",
         " which reached over 380",
         "000 users. Additionally",
         " 531 vulnerable refugees benefited from the Federal Government's relocation strategy",
         " receiving personalized assistance and support in navigating public services.\"",
         "730748",
         "219188"
        ],
        [
         "Brazil",
         "United Nations Sustainable Development Cooperation Framework",
         "2023-01-01",
         "2027-12-31",
         "1510",
         "1",
         "1. Transformação Econômica para o Desenvolvimento Sustentável",
         "2",
         "1.2 Em 2027, o Brasil terá avançado na inclusão econômica que contribui para a redução da pobreza, da fome, das vulnerabilidades, das desigualdades, e da discriminação de geração, gênero, raça e etnia, e que garante o direito à educação transformadora para o pleno desenvolvimento da pessoa e o acesso ao trabalho decente, às oportunidades para geração de renda, à proteção social, econômica e políticas de cuidados e às infraestruturas resilientes, assegurando a igualdade de oportunidades e a sua sustentabilidade.",
         "4",
         "1.2.4 Povos indígenas, povos e comunidades tradicionais e populações do campo, floresta e águas em geral com capacidades fortalecidas para acessar alternativas inovadoras de renda, da preservação e aproveitamento de tradições locais e saberes geracionais para um uso sustentável da sociobiodiversidade, com promoção de produtos locais, da agroecologia, dos sistemas alimentares, do manejo sustentável dos recursos naturais e da pesca sustentável.",
         "-",
         "Yes",
         "1",
         "FAO UTF/BRA/084/BRA Aquaculture Sustainably Developed",
         "134270",
         "Contribute to the sustainable development of Aquaculture by promoting the strengthening of the aquaculture production chain, its diversity and ruling to generate increased production and social inclusion and provide increased income and job offers for low-income populations and other relevant players in the sector.\n",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "Immediate Objective 1: Improve the performance of the aquaculture productive chains.",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "Immediate Objective 2: Set sustainability criteria to the aquaculture parks.",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "Immediate Objective 3: Control quality in the production of water animals and aquaculture products.",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "Immediate Objective 4: Structure monitoring and analysis tools to the National Plan for Aquaculture Monitoring- PNMA.",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "Immediate Objective 5: Restructure the legal framework of the Brazilian aquaculture legislation.",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "Immediate Objective 6: Expand the institutional sustainability of the sector through studies",
         " planning and execution of management actions.\"",
         "2023-01-02",
         "2023-12-31",
         "Closed",
         "-",
         "-",
         "FAO",
         "Food and Agriculture Organization of the United Nations",
         "Brazilian Ministry of Fishing and Aquaculture",
         "Brazilian Ministry of Fishing and Aquaculture",
         "12.2 By 2030, achieve the sustainable management and efficient use of natural resources.",
         "12 Responsible Consumption and Production",
         "Level 0: Brazil",
         "Capacity Development/Technical Assistance",
         "-",
         "-",
         "0 - Not expected to contribute to gender equality/women's empowerment",
         "According to its original design, the Project aimed, in general, to benefit the country's aquaculture sector, through social inclusion initiatives and employment and income generation opportunities related to the aquaculture production chain. As for the potential beneficiaries, they are made up of Brazilian society, governmental and non-governmental organizations, private companies, associations, aquaculture communities, riverside communities, quilombolas, community associations, research centers, extensionists, and professionals working in the sector, and other relevant partners. Due to the discontinuity of the aquaculture parks program, the Project fell short in stimulating occupation and consolidating production in the demarcated aquaculture parks, thus not being able to fulfill its aspiration of including the diversity of the sector, especially about the most vulnerable sectors and social inclusion and also did not adequately incorporate a gender perspective.",
         "1 - Limited contribution to realization of human rights",
         null,
         "0 - Not expected to contribute towards sustaining peace",
         null,
         "Peasants & Rural Workers",
         "Sue Takei; Sergio Dorfler",
         null,
         "8349148",
         "5828949",
         "5747390",
         "8349148",
         "5828949",
         "5747390",
         "Projects activities closed in 2023. The main results of UTF/BRA/084/BRA were surveys/diagnoses of aquaculture productive systems, continental and marine, besides the evaluation of the effectiveness of public programs/policies and improvement of technical and managerial tools to support public activity. The project made important contributions to the Ministry of Fisheries and Aquaculture, mainly focused on planning and development of production chains, aquaculture in Union waters, environmental licensing, and monitoring and management mechanisms, which have certainly brought considerable sectoral gains to increased production and social inclusion intended by the Project, and which will certainly continue to contribute to the expected success of the activity in the coming years. Even without mechanisms that establish a direct correlation between the execution of the Project and the national aquaculture production, there was a considerable increase in national aquaculture production over the duration of the Project: a growth of 36%, from 476,521 to 648,537 tons, respectively – an average of 4% annual growth. Regarding the values generated in primary production, there was a growth of 126% in the same period (annual average of 11% growth), reaching R$ 6.9 billion in 2021 (PPM-IBGE, 2022). Therefore, the Project favored incremental changes that contributed to greater qualification and scope of public policies that benefited national aquaculture, strengthening a base of evidence, analyses, and propositions that allowed conceptual and methodological advances in policies that constitute a legacy to be potentially resumed and deepened. One achievement to be highlighted in 2023 was the Aquaculture Multipliers course, which contributes to improve the quality and effectiveness of public services related to the development of aquaculture, by carrying out a training course aimed at municipal and state public agents working in aquaculture, which covers a wide range of aquaculture-related subjects, providing participants with essential knowledge to become multipliers and promoters of this activity. The course was launched on 13 July 2023 by the Ministry of Fisheries and Aquaculture. This is a partnership between the Ministry, the universities of Brasília (UnB) and Rio Grande do Norte (UFRN) and FAO. It consists of the distribution of essential knowledge about aquaculture, over 160 hours of classes divided into modules. The course is completely free and 100% online. Currently, it has more than three thousand subscribers from Brazil and abroad. The link to the platform is: Plataforma - Multiplicadores Aquicolas (mpa.gov.br).",
         null,
         null,
         null,
         "-",
         null,
         null,
         null,
         "-",
         null,
         null,
         null,
         "-",
         null,
         null,
         null,
         "-",
         "General Population; Quilombola, ribeirinhos and other traditional popualtions",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "Brazil",
         "United Nations Sustainable Development Cooperation Framework",
         "2023-01-01",
         "2027-12-31",
         "1510",
         "1",
         "1. Transformação Econômica para o Desenvolvimento Sustentável",
         "2",
         "1.2 Em 2027, o Brasil terá avançado na inclusão econômica que contribui para a redução da pobreza, da fome, das vulnerabilidades, das desigualdades, e da discriminação de geração, gênero, raça e etnia, e que garante o direito à educação transformadora para o pleno desenvolvimento da pessoa e o acesso ao trabalho decente, às oportunidades para geração de renda, à proteção social, econômica e políticas de cuidados e às infraestruturas resilientes, assegurando a igualdade de oportunidades e a sua sustentabilidade.",
         "1",
         "1.2.1 Governos nacionais e subnacionais, instituições públicas e setor privado têm suas capacidades reforçadas para formular e implementar políticas e iniciativas para promoção do desenvolvimento econômico local, da geração de renda, da ampliação da empregabilidade e garantia do trabalho decente, com foco em igualdade de oportunidades, na redução da discriminação nas relações econômicas e trabalhistas, das diferenças salariais por sexo, geração, gênero, raça, etnia ou nacionalidade, no fortalecimento dos direitos de trabalhadores, dos usuários e das comunidades afetadas pela atividade econômica, e na promoção da inclusão em condições igualitárias de mulheres, jovens, pessoas com deficiência, migrantes e pessoas refugiadas, de modo a proteger os direitos dos segmentos da população em situação de vulnerabilidade.",
         "-",
         "Yes",
         "1",
         "IOM IS.0208 Enhancing Protection, Regularization, and Socio-economic Integration of Migrants and Refugees from Venezuela and Other Relevant Nationalities in the Latin America Region",
         "200514",
         "This project aims to improve access to regularization and documentation processes for vulnerable migrants, strengthen protection mechanisms, and foster long-term integration of migrants into host communities in Acre and Rondônia, in northern Brazil. Particular attention will be given to border towns, such as Assis Brasil, (Acre) and Guajará-Mirim (Rondônia), where protection needs are heightened due to increased patterns of irregular migration and multiple coexisting migration routes.",
         "2025-02-07",
         "2027-02-06",
         "Implementation",
         "-",
         "-",
         "IOM",
         "International Organization for Migration",
         "Global Affairs Canada",
         "#BR Federal Police of Brazil; Brazilian Ministry of Development, Social Assistance, Family and Fight Against Hunger; Brazilian Ministry of Justice and Public Security; Brazilian Public Defenders' Office",
         "5.1 End all forms of discrimination against all women and girls everywhere.,5.2 Eliminate all forms of violence against all women and girls in the public and private spheres, including trafficking and sexual and other types of exploitation.,10.3 Ensure equal opportunity and reduce inequalities of outcome, including by eliminating discriminatory laws, policies and practices and promoting appropriate legislation, policies and action in this regard.,10.7 Facilitate orderly, safe, regular and responsible migration and mobility of people, including through the implementation of planned and well-managed migration policies.,16.2 End abuse, exploitations, trafficking and all forms of violence against and torture of children.",
         "5 Gender Equality; 10 Reduced Inequalities; 16 Peace and Justice - Strong Institutions",
         "Level 0: Brazil (Level 1: Rondônia (RO); Acre (AC))",
         "Direct Support/ Service Delivery; Capacity Development/Technical Assistance",
         "-",
         "-",
         "2 - Gender equality/women's empowerment is a significant objective",
         null,
         "3 - Principal contribution is to the realization of human rights",
         null,
         "0 - Not expected to contribute towards sustaining peace",
         null,
         "-",
         "Fernanda Garcia",
         null,
         "558267",
         "558267",
         "12813",
         null,
         null,
         null,
         "-",
         null,
         null,
         null,
         "-",
         "232859",
         "232859",
         "12813",
         "-",
         "278921",
         "278921",
         null,
         "-",
         "46487",
         "46487",
         null,
         "-",
         "-"
        ],
        [
         "Brazil",
         "United Nations Sustainable Development Cooperation Framework",
         "2023-01-01",
         "2027-12-31",
         "1510",
         "4",
         "4. Governança e Capacidades Institucionais",
         "1",
         "4.1 Em 2027, o Brasil terá fortalecido, no marco do estado democrático de direito, sua governança, legislação, capacidades e articulação institucionais com ampliação da participação popular para elaborar e executar políticas públicas baseadas em evidências, em direitos humanos e igualdade de geração, gênero, raça e etnia, com vistas à prevenção e ao enfrentamento à corrupção, ao crime e às múltiplas formas de violência, e orientadas às especificidades do território e à transparência, com inovação, cooperação nacional e internacional, e ampla participação da sociedade.",
         "5",
         "4.1.5 Capacidades governamentais fortalecidas para garantir uma abordagem multidimensional para combater crimes transnacionais, em especial durante crises humanitárias, e mitigar seus impactos sobre grupos e pessoas em situação de vulnerabilidade, compartilhando informações e aprimorando políticas orientadas por evidências, voltadas ao respeito aos direitos humanos e baseadas na igualdade de geração, gênero, raça e etnia.",
         "-",
         "Yes",
         "1",
         "IOM PX.0650 Improving Reintegration Outcomes for Survivors of Modern Slavery (Phase 2)",
         "200515",
         "This project seeks to address these gaps by enhancing the capacities of key actors to improve voluntary return and reintegration processes and outcomes for survivors of modern slavery and human trafficking. Building on activities conducted in the first phase of the project (PX.0543), it will strengthen the understanding and capacity of key actors in the UK and in countries of return, such as Brazil. This will be done by assessing the role of police and local authorities in the voluntary return of survivors from the UK, as well as by building the capacity of key actors involved in voluntary return and reintegration and conducting or updating national mappings of available reintegration services in countries of return. Second, the project will enhance the engagement of survivors of modern slavery and human trafficking in the process of voluntary return and reintegration, to centre survivor voice and agency, by conducting lived experience research to understand the experiences of survivors who have returned, and by piloting a Virtual Orientation programme for survivors considering return from the UK, promoting survivor agency in decision making.",
         "2025-07-01",
         "2026-03-31",
         "Implementation",
         "-",
         "-",
         "IOM",
         "International Organization for Migration",
         "UK Home Office",
         "UK Home Office",
         "5.1 End all forms of discrimination against all women and girls everywhere.,5.2 Eliminate all forms of violence against all women and girls in the public and private spheres, including trafficking and sexual and other types of exploitation.,8.7 Take immediate and effective measures to eradicate forced labour, end modern slavery and human trafficking and secure the prohibition and elimination of the worst forms of child labour, including recruitment and use of child soldiers, and by 2025 end child labour in all its forms.,10.7 Facilitate orderly, safe, regular and responsible migration and mobility of people, including through the implementation of planned and well-managed migration policies.",
         "5 Gender Equality; 8 Decent Jobs and Economic Growth; 10 Reduced Inequalities",
         "Level 0: Brazil",
         "Capacity Development/Technical Assistance; Direct Support/ Service Delivery",
         "-",
         "-",
         "2 - Gender equality/women's empowerment is a significant objective",
         null,
         "3 - Principal contribution is to the realization of human rights",
         null,
         "0 - Not expected to contribute towards sustaining peace",
         null,
         "Migrants; Victims of grave human rights violations of (slavery, torture, trafficking, sexual exploitation and abuse...)",
         "Fernanda Garcia",
         null,
         "106715",
         "106715",
         "14212",
         null,
         null,
         null,
         "-",
         null,
         null,
         null,
         "-",
         "77191",
         "77191",
         "14212",
         "-",
         "29524",
         "29524",
         null,
         "-",
         null,
         null,
         null,
         "-",
         "-"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Plan name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Plan start date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Plan end date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Plan ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Strategic priority code",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Strategic priority",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Outcome code",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Outcome",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Output code",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Output",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Suboutput also contributes to the following outputs",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sub-Output publication status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sub-Output code",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sub-Output",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Start date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "End date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Joint Programmes / Initiatives names",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Joint Programmes / Initiatives types",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Agency abbreviations",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Agencies",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Contributing partners",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Implementation partners",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "SDG Targets",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "SDG Goals",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Geography",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "QCPR function",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Humanitarian marker",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Humanitarian marker narrative",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Gender marker",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Gender marker narrative",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Human rights marker",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Human rights marker narrative",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sustaining peace marker",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sustaining peace marker narrative",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "LNOB groups targeted",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Focal users",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Non Monetary Assistance",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Total required resources",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Total available resources",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Total expenditure resources",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2023 Required",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2023 Available",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2023 Expenditure",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2023 Narrative",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2024 Required",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2024 Available",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2024 Expenditure",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2024 Narrative",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2025 Required",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2025 Available",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2025 Expenditure",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2025 Narrative",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2026 Required",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2026 Available",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2026 Expenditure",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2026 Narrative",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2027 Required",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2027 Available",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2027 Expenditure",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2027 Narrative",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Brazil - Other beneficiaries",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Ler o arquivo EXPORT_2025_12-13 /Volumes/mvp/default/data/export_2025-12-13.csv/Volumes/mvp/default/data/export_2025-12-13.csvque está no ESQUEMA DEFAULT\n",
    "df=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/Volumes/mvp/default/data/export_2025-12-13.csv\")\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53f79b26-a7c5-4db2-8834-0c772754a56b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8060159725756920>, line 10\u001B[0m\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[ ,;\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m()\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mn\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mt=]\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m, col)\u001B[38;5;241m.\u001B[39mstrip(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m      8\u001B[0m df_clean \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mtoDF(\u001B[38;5;241m*\u001B[39m[sanitize_column(c) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39mcolumns])\n",
       "\u001B[0;32m---> 10\u001B[0m df_clean\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbr_jwp\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py:737\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m    735\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mtable_name \u001B[38;5;241m=\u001B[39m name\n",
       "\u001B[1;32m    736\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mtable_save_method \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msave_as_table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m--> 737\u001B[0m _, _, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(\n",
       "\u001B[1;32m    738\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mobservations\n",
       "\u001B[1;32m    739\u001B[0m )\n",
       "\u001B[1;32m    740\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback(ei)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n",
       "\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1558\u001B[0m )\n",
       "\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   2061\u001B[0m     ):\n",
       "\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2434\u001B[0m                 info,\n",
       "\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2438\u001B[0m                 status_code,\n",
       "\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n",
       "\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: 7d9de85f-7ae1-4d53-904f-cdb4c2498988).\n",
       "To enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n",
       "'.option(\"mergeSchema\", \"true\")'.\n",
       "For other operations, set the session configuration\n",
       "spark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\n",
       "specific to the operation for details.\n",
       "\n",
       "Table schema:\n",
       "root\n",
       "-- Plan_name: string (nullable = true)\n",
       "-- Strategic_priority_code: string (nullable = true)\n",
       "-- Outcome: string (nullable = true)\n",
       "-- Output: string (nullable = true)\n",
       "-- Sub-Output_code: string (nullable = true)\n",
       "-- Sub-Output: string (nullable = true)\n",
       "-- ID: string (nullable = true)\n",
       "-- Start_date: string (nullable = true)\n",
       "-- End_date: string (nullable = true)\n",
       "-- Status: string (nullable = true)\n",
       "-- Joint_Programmes_/_Initiatives_types: string (nullable = true)\n",
       "-- Agencies: string (nullable = true)\n",
       "-- Contributing_partners: string (nullable = true)\n",
       "-- SDG_Targets: string (nullable = true)\n",
       "-- SDG_Goals: string (nullable = true)\n",
       "-- Geography: string (nullable = true)\n",
       "-- QCPR_function: string (nullable = true)\n",
       "-- Gender_marker: string (nullable = true)\n",
       "-- Human_rights_marker: string (nullable = true)\n",
       "-- Sustaining_peace_marker: string (nullable = true)\n",
       "-- LNOB_groups_targeted: string (nullable = true)\n",
       "-- Total_required_resources: string (nullable = true)\n",
       "-- Total_available_resources: string (nullable = true)\n",
       "-- Total_expenditure_resources: string (nullable = true)\n",
       "-- 2023_Required: string (nullable = true)\n",
       "-- 2023_Available: string (nullable = true)\n",
       "-- 2023_Expenditure: string (nullable = true)\n",
       "-- 2024_Required: string (nullable = true)\n",
       "-- 2024_Available: string (nullable = true)\n",
       "-- 2024_Expenditure: string (nullable = true)\n",
       "-- 2025_Required: string (nullable = true)\n",
       "-- 2025_Available: string (nullable = true)\n",
       "-- 2025_Expenditure: string (nullable = true)\n",
       "-- 2026_Required: string (nullable = true)\n",
       "-- 2026_Available: string (nullable = true)\n",
       "-- 2027_Required: string (nullable = true)\n",
       "-- 2027_Available: string (nullable = true)\n",
       "-- Brazil_-_Other_beneficiaries: string (nullable = true)\n",
       "\n",
       "\n",
       "Data schema:\n",
       "root\n",
       "-- Country: string (nullable = true)\n",
       "-- Plan_name: string (nullable = true)\n",
       "-- Plan_start_date: string (nullable = true)\n",
       "-- Plan_end_date: string (nullable = true)\n",
       "-- Plan_ID: string (nullable = true)\n",
       "-- Strategic_priority_code: string (nullable = true)\n",
       "-- Strategic_priority: string (nullable = true)\n",
       "-- Outcome_code: string (nullable = true)\n",
       "-- Outcome: string (nullable = true)\n",
       "-- Output_code: string (nullable = true)\n",
       "-- Output: string (nullable = true)\n",
       "-- Suboutput_also_contributes_to_the_following_outputs: string (nullable = true)\n",
       "-- Sub-Output_publication_status: string (nullable = true)\n",
       "-- Sub-Output_code: string (nullable = true)\n",
       "-- Sub-Output: string (nullable = true)\n",
       "-- ID: string (nullable = true)\n",
       "-- Description: string (nullable = true)\n",
       "-- Start_date: string (nullable = true)\n",
       "-- End_date: string (nullable = true)\n",
       "-- Status: string (nullable = true)\n",
       "-- Joint_Programmes_/_Initiatives_names: string (nullable = true)\n",
       "-- Joint_Programmes_/_Initiatives_types: string (nullable = true)\n",
       "-- Agency_abbreviations: string (nullable = true)\n",
       "-- Agencies: string (nullable = true)\n",
       "-- Contributing_partners: string (nullable = true)\n",
       "-- Implementation_partners: string (nullable = true)\n",
       "-- SDG_Targets: string (nullable = true)\n",
       "-- SDG_Goals: string (nullable = true)\n",
       "-- Geography: string (nullable = true)\n",
       "-- QCPR_function: string (nullable = true)\n",
       "-- Humanitarian_marker: string (nullable = true)\n",
       "-- Humanitarian_marker_narrative: string (nullable = true)\n",
       "-- Gender_marker: string (nullable = true)\n",
       "-- Gender_marker_narrative: string (nullable = true)\n",
       "-- Human_rights_marker: string (nullable = true)\n",
       "-- Human_rights_marker_narrative: string (nullable = true)\n",
       "-- Sustaining_peace_marker: string (nullable = true)\n",
       "-- Sustaining_peace_marker_narrative: string (nullable = true)\n",
       "-- LNOB_groups_targeted: string (nullable = true)\n",
       "-- Focal_users: string (nullable = true)\n",
       "-- Non_Monetary_Assistance: string (nullable = true)\n",
       "-- Total_required_resources: string (nullable = true)\n",
       "-- Total_available_resources: string (nullable = true)\n",
       "-- Total_expenditure_resources: string (nullable = true)\n",
       "-- 2023_Required: string (nullable = true)\n",
       "-- 2023_Available: string (nullable = true)\n",
       "-- 2023_Expenditure: string (nullable = true)\n",
       "-- 2023_Narrative: string (nullable = true)\n",
       "-- 2024_Required: string (nullable = true)\n",
       "-- 2024_Available: string (nullable = true)\n",
       "-- 2024_Expenditure: string (nullable = true)\n",
       "-- 2024_Narrative: string (nullable = true)\n",
       "-- 2025_Required: string (nullable = true)\n",
       "-- 2025_Available: string (nullable = true)\n",
       "-- 2025_Expenditure: string (nullable = true)\n",
       "-- 2025_Narrative: string (nullable = true)\n",
       "-- 2026_Required: string (nullable = true)\n",
       "-- 2026_Available: string (nullable = true)\n",
       "-- 2026_Expenditure: string (nullable = true)\n",
       "-- 2026_Narrative: string (nullable = true)\n",
       "-- 2027_Required: string (nullable = true)\n",
       "-- 2027_Available: string (nullable = true)\n",
       "-- 2027_Expenditure: string (nullable = true)\n",
       "-- 2027_Narrative: string (nullable = true)\n",
       "-- Brazil_-_Other_beneficiaries: string (nullable = true)\n",
       "\n",
       "         \n",
       "To overwrite your schema or change partitioning, please set:\n",
       "'.option(\"overwriteSchema\", \"true\")'.\n",
       "\n",
       "Note that the schema can't be overwritten when using\n",
       "'replaceWhere'.\n",
       "         \n",
       "Table ACLs are enabled in this cluster, so automatic schema migration is not allowed. Please use the ALTER TABLE command for changing the schema.         \n",
       "\n",
       "JVM stacktrace:\n",
       "com.databricks.sql.transaction.tahoe.DeltaAnalysisException\n",
       "\tat com.databricks.sql.transaction.tahoe.MetadataMismatchErrorBuilder.finalizeAndThrow(DeltaErrors.scala:4310)\n",
       "\tat com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata(ImplicitMetadataOperation.scala:231)\n",
       "\tat com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata$(ImplicitMetadataOperation.scala:92)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.updateMetadata(WriteIntoDeltaEdge.scala:120)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitDataAndMaterializationPlans(WriteIntoDeltaEdge.scala:411)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitData(WriteIntoDeltaEdge.scala:256)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:670)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTableAsSelect(CreateDeltaTableCommand.scala:758)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$handleCommit$1(CreateDeltaTableCommand.scala:406)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction$.withActive(OptimisticTransaction.scala:257)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:387)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$11(CreateDeltaTableCommand.scala:310)\n",
       "\tat org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf(SQLConfHelper.scala:75)\n",
       "\tat org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf$(SQLConfHelper.scala:38)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.withSQLConf(QueryPlan.scala:60)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$7(CreateDeltaTableCommand.scala:306)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:493)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:480)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:101)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:194)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:101)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:193)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:29)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:29)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:104)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:194)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:153)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:101)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:192)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:182)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:171)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:101)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:267)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:774)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:152)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:336)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1906)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:152)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1889)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.commitStagedChanges(DataSourceV2Utils.scala:351)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$6(WriteToDataSourceV2Exec.scala:841)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$.handleConcurrentCreateExceptions(WriteToDataSourceV2Exec.scala:77)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:841)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1629)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:828)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:845)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:816)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:270)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:343)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:596)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:596)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:595)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:591)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:587)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:528)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:585)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:702)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:694)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:543)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:543)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:694)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:694)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:489)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:730)\n",
       "\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:854)\n",
       "\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:702)\n",
       "\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:609)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:4068)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3426)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: 7d9de85f-7ae1-4d53-904f-cdb4c2498988).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- Plan_name: string (nullable = true)\n-- Strategic_priority_code: string (nullable = true)\n-- Outcome: string (nullable = true)\n-- Output: string (nullable = true)\n-- Sub-Output_code: string (nullable = true)\n-- Sub-Output: string (nullable = true)\n-- ID: string (nullable = true)\n-- Start_date: string (nullable = true)\n-- End_date: string (nullable = true)\n-- Status: string (nullable = true)\n-- Joint_Programmes_/_Initiatives_types: string (nullable = true)\n-- Agencies: string (nullable = true)\n-- Contributing_partners: string (nullable = true)\n-- SDG_Targets: string (nullable = true)\n-- SDG_Goals: string (nullable = true)\n-- Geography: string (nullable = true)\n-- QCPR_function: string (nullable = true)\n-- Gender_marker: string (nullable = true)\n-- Human_rights_marker: string (nullable = true)\n-- Sustaining_peace_marker: string (nullable = true)\n-- LNOB_groups_targeted: string (nullable = true)\n-- Total_required_resources: string (nullable = true)\n-- Total_available_resources: string (nullable = true)\n-- Total_expenditure_resources: string (nullable = true)\n-- 2023_Required: string (nullable = true)\n-- 2023_Available: string (nullable = true)\n-- 2023_Expenditure: string (nullable = true)\n-- 2024_Required: string (nullable = true)\n-- 2024_Available: string (nullable = true)\n-- 2024_Expenditure: string (nullable = true)\n-- 2025_Required: string (nullable = true)\n-- 2025_Available: string (nullable = true)\n-- 2025_Expenditure: string (nullable = true)\n-- 2026_Required: string (nullable = true)\n-- 2026_Available: string (nullable = true)\n-- 2027_Required: string (nullable = true)\n-- 2027_Available: string (nullable = true)\n-- Brazil_-_Other_beneficiaries: string (nullable = true)\n\n\nData schema:\nroot\n-- Country: string (nullable = true)\n-- Plan_name: string (nullable = true)\n-- Plan_start_date: string (nullable = true)\n-- Plan_end_date: string (nullable = true)\n-- Plan_ID: string (nullable = true)\n-- Strategic_priority_code: string (nullable = true)\n-- Strategic_priority: string (nullable = true)\n-- Outcome_code: string (nullable = true)\n-- Outcome: string (nullable = true)\n-- Output_code: string (nullable = true)\n-- Output: string (nullable = true)\n-- Suboutput_also_contributes_to_the_following_outputs: string (nullable = true)\n-- Sub-Output_publication_status: string (nullable = true)\n-- Sub-Output_code: string (nullable = true)\n-- Sub-Output: string (nullable = true)\n-- ID: string (nullable = true)\n-- Description: string (nullable = true)\n-- Start_date: string (nullable = true)\n-- End_date: string (nullable = true)\n-- Status: string (nullable = true)\n-- Joint_Programmes_/_Initiatives_names: string (nullable = true)\n-- Joint_Programmes_/_Initiatives_types: string (nullable = true)\n-- Agency_abbreviations: string (nullable = true)\n-- Agencies: string (nullable = true)\n-- Contributing_partners: string (nullable = true)\n-- Implementation_partners: string (nullable = true)\n-- SDG_Targets: string (nullable = true)\n-- SDG_Goals: string (nullable = true)\n-- Geography: string (nullable = true)\n-- QCPR_function: string (nullable = true)\n-- Humanitarian_marker: string (nullable = true)\n-- Humanitarian_marker_narrative: string (nullable = true)\n-- Gender_marker: string (nullable = true)\n-- Gender_marker_narrative: string (nullable = true)\n-- Human_rights_marker: string (nullable = true)\n-- Human_rights_marker_narrative: string (nullable = true)\n-- Sustaining_peace_marker: string (nullable = true)\n-- Sustaining_peace_marker_narrative: string (nullable = true)\n-- LNOB_groups_targeted: string (nullable = true)\n-- Focal_users: string (nullable = true)\n-- Non_Monetary_Assistance: string (nullable = true)\n-- Total_required_resources: string (nullable = true)\n-- Total_available_resources: string (nullable = true)\n-- Total_expenditure_resources: string (nullable = true)\n-- 2023_Required: string (nullable = true)\n-- 2023_Available: string (nullable = true)\n-- 2023_Expenditure: string (nullable = true)\n-- 2023_Narrative: string (nullable = true)\n-- 2024_Required: string (nullable = true)\n-- 2024_Available: string (nullable = true)\n-- 2024_Expenditure: string (nullable = true)\n-- 2024_Narrative: string (nullable = true)\n-- 2025_Required: string (nullable = true)\n-- 2025_Available: string (nullable = true)\n-- 2025_Expenditure: string (nullable = true)\n-- 2025_Narrative: string (nullable = true)\n-- 2026_Required: string (nullable = true)\n-- 2026_Available: string (nullable = true)\n-- 2026_Expenditure: string (nullable = true)\n-- 2026_Narrative: string (nullable = true)\n-- 2027_Required: string (nullable = true)\n-- 2027_Available: string (nullable = true)\n-- 2027_Expenditure: string (nullable = true)\n-- 2027_Narrative: string (nullable = true)\n-- Brazil_-_Other_beneficiaries: string (nullable = true)\n\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         \nTable ACLs are enabled in this cluster, so automatic schema migration is not allowed. Please use the ALTER TABLE command for changing the schema.         \n\nJVM stacktrace:\ncom.databricks.sql.transaction.tahoe.DeltaAnalysisException\n\tat com.databricks.sql.transaction.tahoe.MetadataMismatchErrorBuilder.finalizeAndThrow(DeltaErrors.scala:4310)\n\tat com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata(ImplicitMetadataOperation.scala:231)\n\tat com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata$(ImplicitMetadataOperation.scala:92)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.updateMetadata(WriteIntoDeltaEdge.scala:120)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitDataAndMaterializationPlans(WriteIntoDeltaEdge.scala:411)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitData(WriteIntoDeltaEdge.scala:256)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:670)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTableAsSelect(CreateDeltaTableCommand.scala:758)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$handleCommit$1(CreateDeltaTableCommand.scala:406)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction$.withActive(OptimisticTransaction.scala:257)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:387)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$11(CreateDeltaTableCommand.scala:310)\n\tat org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf(SQLConfHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf$(SQLConfHelper.scala:38)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.withSQLConf(QueryPlan.scala:60)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$7(CreateDeltaTableCommand.scala:306)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:493)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:480)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:101)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:194)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:101)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:193)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:104)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:194)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:153)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:101)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:192)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:182)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:171)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:101)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:267)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:774)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:152)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:336)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1906)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:152)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1889)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.commitStagedChanges(DataSourceV2Utils.scala:351)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$6(WriteToDataSourceV2Exec.scala:841)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$.handleConcurrentCreateExceptions(WriteToDataSourceV2Exec.scala:77)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:841)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1629)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:828)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:845)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:816)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:270)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:343)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:596)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:596)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:595)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:591)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:587)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:528)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:585)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:702)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:694)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:519)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:694)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:694)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:489)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:730)\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:854)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:702)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:609)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:4068)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3426)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: 7d9de85f-7ae1-4d53-904f-cdb4c2498988).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- Plan_name: string (nullable = true)\n-- Strategic_priority_code: string (nullable = true)\n-- Outcome: string (nullable = true)\n-- Output: string (nullable = true)\n-- Sub-Output_code: string (nullable = true)\n-- Sub-Output: string (nullable = true)\n-- ID: string (nullable = true)\n-- Start_date: string (nullable = true)\n-- End_date: string (nullable = true)\n-- Status: string (nullable = true)\n-- Joint_Programmes_/_Initiatives_types: string (nullable = true)\n-- Agencies: string (nullable = true)\n-- Contributing_partners: string (nullable = true)\n-- SDG_Targets: string (nullable = true)\n-- SDG_Goals: string (nullable = true)\n-- Geography: string (nullable = true)\n-- QCPR_function: string (nullable = true)\n-- Gender_marker: string (nullable = true)\n-- Human_rights_marker: string (nullable = true)\n-- Sustaining_peace_marker: string (nullable = true)\n-- LNOB_groups_targeted: string (nullable = true)\n-- Total_required_resources: string (nullable = true)\n-- Total_available_resources: string (nullable = true)\n-- Total_expenditure_resources: string (nullable = true)\n-- 2023_Required: string (nullable = true)\n-- 2023_Available: string (nullable = true)\n-- 2023_Expenditure: string (nullable = true)\n-- 2024_Required: string (nullable = true)\n-- 2024_Available: string (nullable = true)\n-- 2024_Expenditure: string (nullable = true)\n-- 2025_Required: string (nullable = true)\n-- 2025_Available: string (nullable = true)\n-- 2025_Expenditure: string (nullable = true)\n-- 2026_Required: string (nullable = true)\n-- 2026_Available: string (nullable = true)\n-- 2027_Required: string (nullable = true)\n-- 2027_Available: string (nullable = true)\n-- Brazil_-_Other_beneficiaries: string (nullable = true)\n\n\nData schema:\nroot\n-- Country: string (nullable = true)\n-- Plan_name: string (nullable = true)\n-- Plan_start_date: string (nullable = true)\n-- Plan_end_date: string (nullable = true)\n-- Plan_ID: string (nullable = true)\n-- Strategic_priority_code: string (nullable = true)\n-- Strategic_priority: string (nullable = true)\n-- Outcome_code: string (nullable = true)\n-- Outcome: string (nullable = true)\n-- Output_code: string (nullable = true)\n-- Output: string (nullable = true)\n-- Suboutput_also_contributes_to_the_following_outputs: string (nullable = true)\n-- Sub-Output_publication_status: string (nullable = true)\n-- Sub-Output_code: string (nullable = true)\n-- Sub-Output: string (nullable = true)\n-- ID: string (nullable = true)\n-- Description: string (nullable = true)\n-- Start_date: string (nullable = true)\n-- End_date: string (nullable = true)\n-- Status: string (nullable = true)\n-- Joint_Programmes_/_Initiatives_names: string (nullable = true)\n-- Joint_Programmes_/_Initiatives_types: string (nullable = true)\n-- Agency_abbreviations: string (nullable = true)\n-- Agencies: string (nullable = true)\n-- Contributing_partners: string (nullable = true)\n-- Implementation_partners: string (nullable = true)\n-- SDG_Targets: string (nullable = true)\n-- SDG_Goals: string (nullable = true)\n-- Geography: string (nullable = true)\n-- QCPR_function: string (nullable = true)\n-- Humanitarian_marker: string (nullable = true)\n-- Humanitarian_marker_narrative: string (nullable = true)\n-- Gender_marker: string (nullable = true)\n-- Gender_marker_narrative: string (nullable = true)\n-- Human_rights_marker: string (nullable = true)\n-- Human_rights_marker_narrative: string (nullable = true)\n-- Sustaining_peace_marker: string (nullable = true)\n-- Sustaining_peace_marker_narrative: string (nullable = true)\n-- LNOB_groups_targeted: string (nullable = true)\n-- Focal_users: string (nullable = true)\n-- Non_Monetary_Assistance: string (nullable = true)\n-- Total_required_resources: string (nullable = true)\n-- Total_available_resources: string (nullable = true)\n-- Total_expenditure_resources: string (nullable = true)\n-- 2023_Required: string (nullable = true)\n-- 2023_Available: string (nullable = true)\n-- 2023_Expenditure: string (nullable = true)\n-- 2023_Narrative: string (nullable = true)\n-- 2024_Required: string (nullable = true)\n-- 2024_Available: string (nullable = true)\n-- 2024_Expenditure: string (nullable = true)\n-- 2024_Narrative: string (nullable = true)\n-- 2025_Required: string (nullable = true)\n-- 2025_Available: string (nullable = true)\n-- 2025_Expenditure: string (nullable = true)\n-- 2025_Narrative: string (nullable = true)\n-- 2026_Required: string (nullable = true)\n-- 2026_Available: string (nullable = true)\n-- 2026_Expenditure: string (nullable = true)\n-- 2026_Narrative: string (nullable = true)\n-- 2027_Required: string (nullable = true)\n-- 2027_Available: string (nullable = true)\n-- 2027_Expenditure: string (nullable = true)\n-- 2027_Narrative: string (nullable = true)\n-- Brazil_-_Other_beneficiaries: string (nullable = true)\n\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         \nTable ACLs are enabled in this cluster, so automatic schema migration is not allowed. Please use the ALTER TABLE command for changing the schema.         \n\nJVM stacktrace:\ncom.databricks.sql.transaction.tahoe.DeltaAnalysisException\n\tat com.databricks.sql.transaction.tahoe.MetadataMismatchErrorBuilder.finalizeAndThrow(DeltaErrors.scala:4310)\n\tat com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata(ImplicitMetadataOperation.scala:231)\n\tat com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata$(ImplicitMetadataOperation.scala:92)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.updateMetadata(WriteIntoDeltaEdge.scala:120)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitDataAndMaterializationPlans(WriteIntoDeltaEdge.scala:411)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitData(WriteIntoDeltaEdge.scala:256)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:670)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTableAsSelect(CreateDeltaTableCommand.scala:758)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$handleCommit$1(CreateDeltaTableCommand.scala:406)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction$.withActive(OptimisticTransaction.scala:257)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:387)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$11(CreateDeltaTableCommand.scala:310)\n\tat org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf(SQLConfHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf$(SQLConfHelper.scala:38)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.withSQLConf(QueryPlan.scala:60)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$7(CreateDeltaTableCommand.scala:306)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:493)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:480)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:101)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:194)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:101)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:193)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:104)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:194)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:153)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:101)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:192)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:182)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:171)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:101)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:267)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:774)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:152)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:336)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1906)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:152)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1889)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.commitStagedChanges(DataSourceV2Utils.scala:351)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$6(WriteToDataSourceV2Exec.scala:841)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$.handleConcurrentCreateExceptions(WriteToDataSourceV2Exec.scala:77)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:841)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1629)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:828)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:845)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:816)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:270)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:343)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:596)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:596)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:595)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:591)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:587)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:528)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:585)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:702)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:694)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:519)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:694)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:694)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:489)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:730)\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:854)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:702)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:609)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:4068)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3426)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": null,
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "XXKCM",
        "stackTrace": "com.databricks.sql.transaction.tahoe.DeltaAnalysisException\n\tat com.databricks.sql.transaction.tahoe.MetadataMismatchErrorBuilder.finalizeAndThrow(DeltaErrors.scala:4310)\n\tat com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata(ImplicitMetadataOperation.scala:231)\n\tat com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata$(ImplicitMetadataOperation.scala:92)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.updateMetadata(WriteIntoDeltaEdge.scala:120)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitDataAndMaterializationPlans(WriteIntoDeltaEdge.scala:411)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitData(WriteIntoDeltaEdge.scala:256)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:670)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTableAsSelect(CreateDeltaTableCommand.scala:758)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$handleCommit$1(CreateDeltaTableCommand.scala:406)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction$.withActive(OptimisticTransaction.scala:257)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:387)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$11(CreateDeltaTableCommand.scala:310)\n\tat org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf(SQLConfHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf$(SQLConfHelper.scala:38)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.withSQLConf(QueryPlan.scala:60)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$7(CreateDeltaTableCommand.scala:306)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:493)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:480)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:101)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:194)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:101)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:193)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:104)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:194)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:153)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:101)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:192)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:182)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:171)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:101)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:267)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:774)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:152)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:336)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1906)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:152)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1889)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.commitStagedChanges(DataSourceV2Utils.scala:351)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$6(WriteToDataSourceV2Exec.scala:841)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$.handleConcurrentCreateExceptions(WriteToDataSourceV2Exec.scala:77)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:841)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1629)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:828)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:845)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:816)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:270)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:343)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:596)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:596)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:595)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:591)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:587)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:528)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:585)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:702)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:694)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:519)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:694)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:694)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:489)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:730)\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:854)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:702)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:609)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:4068)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3426)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-8060159725756920>, line 10\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[ ,;\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m()\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mn\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mt=]\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m, col)\u001B[38;5;241m.\u001B[39mstrip(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      8\u001B[0m df_clean \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mtoDF(\u001B[38;5;241m*\u001B[39m[sanitize_column(c) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39mcolumns])\n\u001B[0;32m---> 10\u001B[0m df_clean\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbr_jwp\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py:737\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m    735\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mtable_name \u001B[38;5;241m=\u001B[39m name\n\u001B[1;32m    736\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mtable_save_method \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msave_as_table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 737\u001B[0m _, _, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(\n\u001B[1;32m    738\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mobservations\n\u001B[1;32m    739\u001B[0m )\n\u001B[1;32m    740\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback(ei)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1558\u001B[0m )\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   2061\u001B[0m     ):\n\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2434\u001B[0m                 info,\n\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2438\u001B[0m                 status_code,\n\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: 7d9de85f-7ae1-4d53-904f-cdb4c2498988).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- Plan_name: string (nullable = true)\n-- Strategic_priority_code: string (nullable = true)\n-- Outcome: string (nullable = true)\n-- Output: string (nullable = true)\n-- Sub-Output_code: string (nullable = true)\n-- Sub-Output: string (nullable = true)\n-- ID: string (nullable = true)\n-- Start_date: string (nullable = true)\n-- End_date: string (nullable = true)\n-- Status: string (nullable = true)\n-- Joint_Programmes_/_Initiatives_types: string (nullable = true)\n-- Agencies: string (nullable = true)\n-- Contributing_partners: string (nullable = true)\n-- SDG_Targets: string (nullable = true)\n-- SDG_Goals: string (nullable = true)\n-- Geography: string (nullable = true)\n-- QCPR_function: string (nullable = true)\n-- Gender_marker: string (nullable = true)\n-- Human_rights_marker: string (nullable = true)\n-- Sustaining_peace_marker: string (nullable = true)\n-- LNOB_groups_targeted: string (nullable = true)\n-- Total_required_resources: string (nullable = true)\n-- Total_available_resources: string (nullable = true)\n-- Total_expenditure_resources: string (nullable = true)\n-- 2023_Required: string (nullable = true)\n-- 2023_Available: string (nullable = true)\n-- 2023_Expenditure: string (nullable = true)\n-- 2024_Required: string (nullable = true)\n-- 2024_Available: string (nullable = true)\n-- 2024_Expenditure: string (nullable = true)\n-- 2025_Required: string (nullable = true)\n-- 2025_Available: string (nullable = true)\n-- 2025_Expenditure: string (nullable = true)\n-- 2026_Required: string (nullable = true)\n-- 2026_Available: string (nullable = true)\n-- 2027_Required: string (nullable = true)\n-- 2027_Available: string (nullable = true)\n-- Brazil_-_Other_beneficiaries: string (nullable = true)\n\n\nData schema:\nroot\n-- Country: string (nullable = true)\n-- Plan_name: string (nullable = true)\n-- Plan_start_date: string (nullable = true)\n-- Plan_end_date: string (nullable = true)\n-- Plan_ID: string (nullable = true)\n-- Strategic_priority_code: string (nullable = true)\n-- Strategic_priority: string (nullable = true)\n-- Outcome_code: string (nullable = true)\n-- Outcome: string (nullable = true)\n-- Output_code: string (nullable = true)\n-- Output: string (nullable = true)\n-- Suboutput_also_contributes_to_the_following_outputs: string (nullable = true)\n-- Sub-Output_publication_status: string (nullable = true)\n-- Sub-Output_code: string (nullable = true)\n-- Sub-Output: string (nullable = true)\n-- ID: string (nullable = true)\n-- Description: string (nullable = true)\n-- Start_date: string (nullable = true)\n-- End_date: string (nullable = true)\n-- Status: string (nullable = true)\n-- Joint_Programmes_/_Initiatives_names: string (nullable = true)\n-- Joint_Programmes_/_Initiatives_types: string (nullable = true)\n-- Agency_abbreviations: string (nullable = true)\n-- Agencies: string (nullable = true)\n-- Contributing_partners: string (nullable = true)\n-- Implementation_partners: string (nullable = true)\n-- SDG_Targets: string (nullable = true)\n-- SDG_Goals: string (nullable = true)\n-- Geography: string (nullable = true)\n-- QCPR_function: string (nullable = true)\n-- Humanitarian_marker: string (nullable = true)\n-- Humanitarian_marker_narrative: string (nullable = true)\n-- Gender_marker: string (nullable = true)\n-- Gender_marker_narrative: string (nullable = true)\n-- Human_rights_marker: string (nullable = true)\n-- Human_rights_marker_narrative: string (nullable = true)\n-- Sustaining_peace_marker: string (nullable = true)\n-- Sustaining_peace_marker_narrative: string (nullable = true)\n-- LNOB_groups_targeted: string (nullable = true)\n-- Focal_users: string (nullable = true)\n-- Non_Monetary_Assistance: string (nullable = true)\n-- Total_required_resources: string (nullable = true)\n-- Total_available_resources: string (nullable = true)\n-- Total_expenditure_resources: string (nullable = true)\n-- 2023_Required: string (nullable = true)\n-- 2023_Available: string (nullable = true)\n-- 2023_Expenditure: string (nullable = true)\n-- 2023_Narrative: string (nullable = true)\n-- 2024_Required: string (nullable = true)\n-- 2024_Available: string (nullable = true)\n-- 2024_Expenditure: string (nullable = true)\n-- 2024_Narrative: string (nullable = true)\n-- 2025_Required: string (nullable = true)\n-- 2025_Available: string (nullable = true)\n-- 2025_Expenditure: string (nullable = true)\n-- 2025_Narrative: string (nullable = true)\n-- 2026_Required: string (nullable = true)\n-- 2026_Available: string (nullable = true)\n-- 2026_Expenditure: string (nullable = true)\n-- 2026_Narrative: string (nullable = true)\n-- 2027_Required: string (nullable = true)\n-- 2027_Available: string (nullable = true)\n-- 2027_Expenditure: string (nullable = true)\n-- 2027_Narrative: string (nullable = true)\n-- Brazil_-_Other_beneficiaries: string (nullable = true)\n\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         \nTable ACLs are enabled in this cluster, so automatic schema migration is not allowed. Please use the ALTER TABLE command for changing the schema.         \n\nJVM stacktrace:\ncom.databricks.sql.transaction.tahoe.DeltaAnalysisException\n\tat com.databricks.sql.transaction.tahoe.MetadataMismatchErrorBuilder.finalizeAndThrow(DeltaErrors.scala:4310)\n\tat com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata(ImplicitMetadataOperation.scala:231)\n\tat com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata$(ImplicitMetadataOperation.scala:92)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.updateMetadata(WriteIntoDeltaEdge.scala:120)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitDataAndMaterializationPlans(WriteIntoDeltaEdge.scala:411)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitData(WriteIntoDeltaEdge.scala:256)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:670)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTableAsSelect(CreateDeltaTableCommand.scala:758)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$handleCommit$1(CreateDeltaTableCommand.scala:406)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction$.withActive(OptimisticTransaction.scala:257)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:387)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$11(CreateDeltaTableCommand.scala:310)\n\tat org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf(SQLConfHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf$(SQLConfHelper.scala:38)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.withSQLConf(QueryPlan.scala:60)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$7(CreateDeltaTableCommand.scala:306)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:493)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:480)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:101)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:194)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:101)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:193)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:104)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:194)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:153)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:101)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:192)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:182)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:171)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:101)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:267)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:774)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:152)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:336)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1906)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:152)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1889)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.commitStagedChanges(DataSourceV2Utils.scala:351)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$6(WriteToDataSourceV2Exec.scala:841)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$.handleConcurrentCreateExceptions(WriteToDataSourceV2Exec.scala:77)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:841)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1629)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:828)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:845)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:816)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:270)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:343)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:596)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:596)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:595)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:591)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:587)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:528)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:585)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:702)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:694)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:519)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:694)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:694)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:489)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:730)\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:854)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:702)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:609)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:4068)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3426)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Aplicar esta primeira transformação no nome das colunas porque havia caracterers não aceitos no formato Delta Table\n",
    "import re\n",
    "\n",
    "def sanitize_column(col):\n",
    "    # Replace invalid characters with underscores and strip leading/trailing spaces\n",
    "    return re.sub(r'[ ,;{}()\\n\\t=]', '_', col).strip('_')\n",
    "\n",
    "df_clean = df.toDF(*[sanitize_column(c) for c in df.columns])\n",
    "\n",
    "df_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"br_jwp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7027117a-c93e-4897-95a7-a8aa4eba8647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "ALTER TABLE mvp.bronze.br_jwp SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3daba538-9ed4-4e8a-9135-ec2a34351f77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS Country;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Plan start date`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Plan end date`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d3d383d-cd0f-41b3-91e5-39d8f479b9cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Plan_ID`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Plan_start_date`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Plan_end_date`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Strategic_priority`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Outcome_code`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Output_code`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Suboutput_also_contributes_to_the_following_outputs`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Sub-Output_publication_status`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Sub-Output _ode`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Description`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Joint_Programmes_/_Initiatives_names`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Joint Programmes_/_Initiatives_types`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Agency_abbreviations`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Implementation_partners`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Humanitarian_marker`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Humanitarian_marker_narrative`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Gender_marker_narrative`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Human_rights_marker_narrative`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Sustaining_peace_marker_narrative`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Focal_users`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `Non_Monetary_Assistance`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `2023_Narrative`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `2024_Narrative`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `2025_Narrative`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `2026_Narrative`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `2027_Narrative`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `2026_Expenditure`;\n",
    "ALTER TABLE mvp.bronze.br_jwp DROP COLUMN IF EXISTS `2027_Expenditure`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "848d05d0-1d16-4656-9228-5e437919c99d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Averiguando o número de colunas após a primeira limpeza\n",
    "df_bronze = spark.table(\"mvp.bronze.br_jwp\")\n",
    "num_columns = len(df_bronze.columns)\n",
    "display(num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9053101-a0b9-41bf-ac1e-3b4d490fe49d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Listar todas as colunas da tabela br_jwp\n",
    "df_bronze = spark.table(\"mvp.bronze.br_jwp\")\n",
    "display(df_bronze.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ea0c8d2-402e-4b54-bcd3-f43311f6dca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Concluída esta etapa bronze e selecionadas as colunas necessárias para o exercício, é possível criar o Catálogo de Dados de forma mais apropriada. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "437b1a0f-5104-461b-8b73-98095373c375",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN Plan_name COMMENT 'Nome do plano, que ajuda a identificar o Marco de Cooperação vigente. O plano atual disponível para o Brasil é o “United Nations Sustainable Development Cooperation Framework”'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN Strategic_priority_code COMMENT 'Código que representa a prioridade estratégica associada ao plano. Cada código representa uma prioridade estratégica.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN Outcome COMMENT 'Descrição dos resultados esperados, detalhando o que o plano pretende alcançar.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN Output COMMENT 'Descrição dos resultados, esclarecendo os produtos específicos produzidos.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN `Sub-Output` COMMENT 'Descrição do subresultado, detalhando suas contribuições específicas para o plano geral.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN ID COMMENT 'identificador único (pk) para o registro, garantindo que cada entrada possa ser referenciada de forma distinta.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN Start_date COMMENT 'Data em que o sub-output específico do plano tem início.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN End_date COMMENT 'Data em que o sub-output específico do plano se encerra.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN Status COMMENT 'O status atual do plano ou de seus componentes, indicando o progresso ou a conclusão. Opções de resposta: implementation, closed, terminated, susepended.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN `Joint_Programmes_/_Initiatives_types` COMMENT 'Nomes de iniciativas conjuntas associada ao plano, se houver.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN Agencies COMMENT 'Nome completo da agência, fundo ou programa responsável pela implementação do sub-output'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN Contributing_partners COMMENT 'Parceiro responsável pela concessão de recursos para a execução do sub-output.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN SDG_Targets COMMENT 'Meta(s) do Desenvolvimento Sustentável associadas ao sub-output específico. Todas as metas podem ser encontradas em: https://sdgs.un.org/goals. Atributo multi-valorado.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN SDG_Goals COMMENT 'Objetivo(s) do Desenvolvimento Sustentável associado(s) ao sub-output específico. Todos os Objetivos podem ser encontrados em: https://sdgs.un.org/goals, Atributo multi-valorado.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN Geography COMMENT 'Área geográfica de implementação. Atributo multi-valorado, aceitando informações em Nível 1 (Brasil), Nível 2 (Estado) e Nível 3 (Município). É possível selecionar mais de uma opção em cada nível'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN QCPR_function COMMENT 'Associação com as funções informadas no Quadrennial Comprehensive Policy Review (QCPR). A descrição completa de cada função pode ser encontrada em: https://help.uninfo.org/un-info/results-framework/results-framework-structure/suboutput-level/guidance-on-applying-tags-and-markers/guidance-for-applying-the-qcpr-functions. '\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN Gender_marker COMMENT 'Identificação de marcador de gênero. A descrição de cada marcador pode ser encontrada em: https://help.uninfo.org/un-info/results-framework/results-framework-structure/suboutput-level/guidance-on-applying-tags-and-markers/guidance-on-applying-the-gender-equality-human-rights-and-sustaining-peace-markers'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN Human_rights_marker COMMENT 'Identificação de marcador de direitos humanos. A descrição de cada marcador pode ser encontrada em: https://help.uninfo.org/un-info/results-framework/results-framework-structure/suboutput-level/guidance-on-applying-tags-and-markers/guidance-on-applying-the-gender-equality-human-rights-and-sustaining-peace-markers'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN Sustaining_peace_marker COMMENT 'Identificação de marcador de paz sustentável. A descrição de cada marcador pode ser encontrada em: https://help.uninfo.org/un-info/results-framework/results-framework-structure/suboutput-level/guidance-on-applying-tags-and-markers/guidance-on-applying-the-gender-equality-human-rights-and-sustaining-peace-markers'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN LNOB_groups_targeted COMMENT 'Identificação multi-valorada de grupos populacionais beneficiados pelo sub-output. Caso a opção \"Other\" seja selecionada, deve-se observar o atributo \"Brazil - other beneficiaries'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN Total_required_resources COMMENT 'Recursos totais necessários para implementar o plano de forma eficaz.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN Total_available_resources COMMENT 'Recursos totais disponíveis para implementar o plano de forma eficaz.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN Total_expenditure_resources COMMENT 'Recursos totais utilizados para implementar o plano de forma eficaz.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN 2023_Required COMMENT 'Recursos totais necessários para implementar as iniciativas previstas em 2023 de forma eficaz.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN 2023_Available COMMENT 'Recursos totais disponíveis para implementar as iniciativas previstas em 2023 de forma eficaz.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN 2023_Expenditure COMMENT 'Recursos totais utilizados em 2023.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN 2024_Required COMMENT 'Recursos totais necessários para implementar as iniciativas previstas em 2024 de forma eficaz.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN 2024_Available COMMENT 'Recursos totais disponíveis para implementar as iniciativas previstas em 2024 de forma eficaz.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN 2024_Expenditure COMMENT 'Recursos totais utilizados em 2024.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN 2025_Required COMMENT 'Recursos totais necessários para implementar as iniciativas previstas em 2025 de forma eficaz.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN 2025_Available COMMENT 'Recursos totais disponíveis para implementar as iniciativas previstas em 2025 de forma eficaz.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN 2025_Expenditure COMMENT 'Recursos totais utilizados em 2025.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN 2026_Required COMMENT 'Recursos totais necessários para implementar as iniciativas previstas em 2026 de forma eficaz.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN 2026_Available COMMENT 'Recursos totais disponíveis para implementar as iniciativas previstas em 2026 de forma eficaz.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN 2027_Required COMMENT 'Recursos totais necessários para implementar as iniciativas previstas em 2027 de forma eficaz.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN 2027_Available COMMENT 'Recursos totais disponíveis para implementar as iniciativas previstas em 2027 de forma eficaz.'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE mvp.bronze.br_jwp \n",
    "ALTER COLUMN `Brazil_-_Other_beneficiaries` COMMENT 'Demais grupos populacionais beneficiados pela iniciativa.'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dff41b3-b1d7-4fb8-800e-3d969334b0b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Catálogo de dados devidamente atualizado.\n",
    "\n",
    "> Figura 5 - Catálogo de dados atualizado\n",
    "> ![](/Workspace/Users/rsousa.rodrigo@gmail.com/mvp-engenharia-de-dados/Figura 5 - Catálogo de dados criado.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70cfccd0-5406-480f-b09d-4766cee621d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6.2 Refinamento PRATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c7e1975-2959-467e-8780-a64395dbb37c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Definir o uso padrão do ESQUEMA PRATA\n",
    "spark.sql(\"USE CATALOG mvp\")\n",
    "spark.sql(\"USE SCHEMA silver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a07fda9-ca3b-4195-bcaf-2a155d0754d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT COUNT(*) AS num_linhas \n",
    "    FROM mvp.bronze.br_jwp\n",
    "    \"\"\"\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a03ae7dc-7a61-4d02-bb0a-1e9b20d46592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A seguir, irei formatar alguns tipos de dados (Strategic Priority Code, datas e valores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "996fc833-ef6a-4fb8-bbe2-9e5350fc37af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, expr, col, try_to_date\n",
    "\n",
    "df_bronze = spark.table(\"mvp.bronze.br_jwp\")\n",
    "\n",
    "columns_int = [\"Strategic_priority_code\"]\n",
    "columns_date = [\"Start_date\", \"End_date\"]\n",
    "columns_float = [\n",
    "    \"Total_required_resources\", \"Total_available_resources\",\n",
    "    \"2023_Required\", \"2023_Available\", \"2023_Expenditure\",\n",
    "    \"2024_Required\", \"2024_Available\", \"2024_Expenditure\",\n",
    "    \"2025_Required\", \"2025_Available\", \"2025_Expenditure\",\n",
    "    \"2026_Required\", \"2026_Available\",\n",
    "    \"2027_Required\", \"2027_Available\"\n",
    "]\n",
    "\n",
    "df_casted = df_bronze\n",
    "for col_name in columns_int:\n",
    "    df_casted = df_casted.withColumn(\n",
    "        col_name,\n",
    "        when(\n",
    "            expr(f\"try_cast({col_name} as int)\").isNotNull(),\n",
    "            expr(f\"try_cast({col_name} as int)\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "\n",
    "for col_name in columns_date:\n",
    "    df_casted = df_casted.withColumn(\n",
    "        col_name,\n",
    "        try_to_date(col(col_name), \"yyyy-MM-dd\")\n",
    "    )\n",
    "\n",
    "for col_name in columns_float:\n",
    "    df_casted = df_casted.withColumn(\n",
    "        col_name,\n",
    "        when(\n",
    "            expr(f\"try_cast({col_name} as float)\").isNotNull(),\n",
    "            expr(f\"try_cast({col_name} as float)\")\n",
    "        ).otherwise(0.00)\n",
    "    )\n",
    "\n",
    "df_casted.write.format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"mvp.silver.br_jwp_silver\")\n",
    "\n",
    "display(df_casted.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89ce2ed3-0d1a-4f68-b77b-4992da0aef96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Para fazer o tratamento dos valores nulos, excluirei todas as linhas cujos valores em \"Plan name\" sejam diferentes de \"United Nations Sustainable Development Cooperation Framework\", bem como os valores em \"Strategic Priority code\" sejam iguais a 0. Como o Portal de Dados ainda está em versão beta, informações foram sendo inseridas de forma diferente a cada atualização do banco de dados, gerando inconsistências na forma como as informações aparecem.\n",
    "\n",
    "Vale destacar que esta alteração poderá influenciar, no final do exercício, diferenças de valores nos atributos Required/Avaiable/Executed em comparação com a base de dados original porque, embora as informaçoes em todas as outras colunas estejam erradas, os valores podem ter sido inseridos corretamente. Entretanto, esse refinamento é essencial para o exercício."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af75563a-8ce2-431d-8b97-d13e3963eab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "DELETE FROM mvp.silver.br_jwp_silver\n",
    "WHERE Plan_name <> 'United Nations Sustainable Development Cooperation Framework'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9307c71-85a9-4774-b405-ab43d7d67ed9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DELETE FROM mvp.silver.br_jwp_silver\n",
    "WHERE Strategic_priority_code = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fbc3b44-46a5-45cc-b1a2-64617a75aa4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Observa-se que também há vários dados com informação NULL na data de início ou término. Isso também simboliza que os dados foram originalmente inseridos em uma versão antiga da base de dados.\n",
    "Por não ser possível inferir a data correta de início ou fim, o que também impede a categorização do status da iniciativa, estes valores também serão excluídos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d45a528e-9462-47c5-8bfc-abe4f4046a19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DELETE FROM mvp.silver.br_jwp_silver\n",
    "WHERE Start_date IS NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc755fcc-cc10-4f4f-a1b7-248255421024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A etapa de arquitetura PRATA está concluída. Neste momento, conseguimos garantir maior qualidade dos dados, que se apresentam de forma mais limpa e validada. Ainda são necessários ajustes em termos de estrutura da tabela - esta modelagem será feita na camada Ouro, na qual os objetos multivalorados serão ajustados.\n",
    "Também nesta etapa prata, foi conduzida uma limpeza de dados inconsistentes e nulos, bem como padronização dos tipos de informação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f77a062-c709-4a37-8845-bdfc78b2916a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6.3 Refinamento OURO\n",
    "Nesta última etapa de transformação dos dados, criarei diferentes tabelas FATO e DIMENSÃO para poder dividir e explodir atributos multivalorados. \n",
    "\n",
    "O desafio est[a] conseguir fracionar e distribuir os recursos orçamentários de acordo com os ODS, área geográfica e beneficiários. Para tanto, serão criadas as seguintes tabelas:\n",
    "- Fato_sub-output\n",
    "- Dim_ODS\n",
    "- Dim_beneficiarios\n",
    "- Dim_ano\n",
    "- Dim_geografia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a22d99b0-2deb-488b-be22-7849a3af9162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Definir o uso padrão do ESQUEMA OURO\n",
    "spark.sql(\"USE CATALOG mvp\")\n",
    "spark.sql(\"USE SCHEMA gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87e1c666-122b-4069-aa5a-ba12402ec7f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Antes de criar as tabelas Dimensão, é preciso normalizar os anos através do UNIPIVOT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53819a00-89b2-419f-b3d3-e4355049e585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE mvp.gold.year_normalized AS\n",
    "with expenditures_unipivot AS (\n",
    "  select\n",
    "    ID,\n",
    "    `Sub-Output`,\n",
    "    SDG_Goals,\n",
    "    2023 AS year,\n",
    "    2023_Expenditure AS Expenditures\n",
    "  FROM mvp.silver.br_jwp_silver\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  select\n",
    "    ID,\n",
    "    `Sub-Output`,\n",
    "    SDG_Goals,\n",
    "    2024 AS year,\n",
    "    2024_Expenditure AS Expenditures\n",
    "  FROM mvp.silver.br_jwp_silver\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  select\n",
    "    ID,\n",
    "    `Sub-Output`,\n",
    "    SDG_Goals,\n",
    "    2025 AS year,\n",
    "    2025_Expenditure AS Expenditures\n",
    "  FROM mvp.silver.br_jwp_silver   \n",
    ")\n",
    "select *\n",
    "from expenditures_unipivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6869aed4-426e-4044-93b5-e7a6d72f7115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql \n",
    "CREATE OR REPLACE TABLE mvp.gold.sdg_exploded AS\n",
    "SELECT\n",
    "  ID,\n",
    "  `Sub-Output`,\n",
    "  year,\n",
    "  Expenditures,\n",
    "  TRIM(sdg_goal) AS SDG\n",
    "FROM mvp.gold.year_normalized\n",
    "LATERAL VIEW explode(split(SDG_Goals, ';')) AS sdg_goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d1efa67-5879-460a-becb-1b1bc4e1cba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Passo 3 - Contar ODS por projeto e ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05968908-d386-4b21-b290-8682b4dbabde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "WITH contagem_ODS AS (\n",
    "  SELECT\n",
    "    ID,\n",
    "    Year,\n",
    "    Count(DISTINCT sdg) AS qtd_SDG\n",
    "  FROM mvp.gold.sdg_exploded\n",
    "  GROUP BY ID, Year\n",
    ")\n",
    "select * from contagem_ODS LIMIT 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "343eb03c-41e6-4d6c-a3bd-7d19fc0ce828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE mvp.gold.count_SDG AS\n",
    "WITH count_SDG AS (\n",
    "  SELECT\n",
    "    ID,\n",
    "    Year,\n",
    "    Count(DISTINCT sdg) AS qtd_SDG\n",
    "  FROM mvp.gold.sdg_exploded\n",
    "  GROUP BY ID, Year\n",
    ")\n",
    "SELECT *\n",
    "FROM count_SDG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3441487f-784e-4a43-998d-74acba81b3ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Passo 4 - Distribuir os valores gastos de acordo com os ODS (sem pesos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ae987ab-e27e-4236-b955-2c13e7290daa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  o.ID,\n",
    "  o.`Sub-Output`,\n",
    "  o.Year,\n",
    "  o.SDG,\n",
    "  o.Expenditures / c.qtd_SDG AS expenditures_per_SDG\n",
    "FROM mvp.gold.sdg_exploded o\n",
    "JOIN mvp.gold.count_SDG c\n",
    "  ON o.ID = c.ID\n",
    "  AND o.Year = c.Year\n",
    "WHERE o.Expenditures IS NOT NULL LIMIT 5; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b71d1e5-182f-4958-ad62-a710332b412b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = (\n",
    "    spark.table(\"mvp.gold.sdg_exploded\")\n",
    "    .join(\n",
    "        spark.table(\"mvp.silver.br_jwp_silver\").select(\"ID\", \"Agencies\"),\n",
    "        on=\"ID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"mvp.gold.fact_expenditure_project_sdg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca21292c-114a-4130-b97d-76c02f9477cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Foi necessário dropar a tabela fact_expenditure_project_sdg e crIar uma nova em razão do tipo de dado configurado na coluna de valores - estava em DOUBLE e erra necessáro estar em decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "788fda60-a0db-4e2d-b755-992f38607f12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE TABLE mvp.gold.fact_expenditure_project_sdg_new (\n",
    "  ID STRING,\n",
    "  `Sub-Output` STRING,\n",
    "  year INT,\n",
    "  Expenditures DECIMAL(18,2),\n",
    "  SDG STRING,\n",
    "  Agencies STRING\n",
    ");\n",
    "\n",
    "INSERT INTO mvp.gold.fact_expenditure_project_sdg_new\n",
    "SELECT\n",
    "  ID,\n",
    "  `Sub-Output`,\n",
    "  year,\n",
    "  CAST(Expenditures AS DECIMAL(18,2)),\n",
    "  SDG,\n",
    "  Agencies\n",
    "FROM mvp.gold.fact_expenditure_project_sdg;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3400d2fe-2e7b-420c-88bf-0093e95c0968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "drop table  mvp.gold.fact_expenditure_project_sdg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56cadc61-e0d8-40aa-9f99-fe1d4762cd84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Criando a tabela dimensão PROJETO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcf160e5-53f3-429e-946a-6c3cc4906443",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Criando a dimensão TEMPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29a1bed5-6dd7-4b3a-84aa-d60353b52532",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "create table if not exists mvp.gold.dim_tempo (\n",
    "  sk_tempo BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "  ano INT,\n",
    "  PRIMARY KEY (sk_tempo)\n",
    ");\n",
    "\n",
    "INSERT INTO mvp.gold.dim_tempo (ano)\n",
    "SELECT DISTINCT year\n",
    "FROM mvp.gold.fact_expenditure_project_sdg_new;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76c8bc05-7931-456f-9572-db2bbd15656c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Dimensão SDG\n",
    "from pyspark.sql.functions import explode, split, trim, ltrim, col, expr, monotonically_increasing_id\n",
    "\n",
    "df = spark.table(\"mvp.silver.br_jwp_silver\").select(\"ID\", \"SDG_Goals\")\n",
    "\n",
    "df_sdg = (\n",
    "    df.withColumn(\"SDG\", explode(split(col(\"SDG_Goals\"), \";\")))\n",
    "      .withColumn(\"SDG\", ltrim(col(\"SDG\")))\n",
    "      .select(\"ID\", \"SDG\")\n",
    "      .withColumn(\"sdg_sk\", expr(\"uuid()\"))\n",
    ")\n",
    "\n",
    "df_sdg = df_sdg.select(\"sdg_sk\", \"ID\", \"SDG\")\n",
    "\n",
    "df_sdg.write.mode(\"overwrite\").saveAsTable(\"mvp.gold.dim_sdg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37ef2b80-54c6-46f7-8da4-1ae80b7193dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Dimensão CONTRIBUTING PARTNER\n",
    "from pyspark.sql.functions import explode, split, ltrim, col, expr\n",
    "\n",
    "df = spark.table(\"mvp.silver.br_jwp_silver\").select(\"ID\", \"Contributing_partners\")\n",
    "\n",
    "df_partners = (\n",
    "    df.withColumn(\"Partner\", explode(split(col(\"Contributing_partners\"), \";\")))\n",
    "      .withColumn(\"Partner\", ltrim(col(\"Partner\")))\n",
    "      .withColumn(\"sk_partner\", expr(\"uuid()\"))\n",
    "      .select(\"sk_partner\", \"ID\", \"Partner\")\n",
    ")\n",
    "\n",
    "df_partners.write.mode(\"overwrite\").saveAsTable(\"mvp.gold.dim_contributing_partners\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6ae0eb2-1c38-4f15-88b3-f3aa0e03dc6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "AlterTable"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "ALTER TABLE mvp.gold.dim_contributing_partners \n",
    "ALTER COLUMN sk_partner SET NOT NULL;\n",
    "\n",
    "ALTER TABLE mvp.gold.dim_contributing_partners \n",
    "ADD PRIMARY KEY (sk_partner);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "965f8a74-7f0b-4099-bd41-025d1f5c702f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Dimensão AGENCY\n",
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "df_agency = (\n",
    "    spark.table(\"mvp.silver.br_jwp_silver\")\n",
    "    .select(\"Agencies\")\n",
    "    .distinct()\n",
    "    .withColumn(\"sk_agency\", expr(\"uuid()\"))\n",
    "    .select(\"sk_agency\", col(\"Agencies\").alias(\"Agency\"))\n",
    ")\n",
    "\n",
    "df_agency.write.mode(\"overwrite\").saveAsTable(\"mvp.gold.dim_agency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63927384-48c3-462c-ae4d-17f4114ab4a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Dimensão PROJECT\n",
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "df_project = (\n",
    "    spark.table(\"mvp.silver.br_jwp_silver\")\n",
    "    .select(\n",
    "        expr(\"uuid()\").alias(\"sk_project\"),\n",
    "        col(\"ID\").cast(\"int\").alias(\"ID_Projeto\"),\n",
    "        col(\"`Sub-Output`\").alias(\"Sub_output\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_project.write.mode(\"overwrite\").saveAsTable(\"mvp.gold.dim_project_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "718c424e-805a-4eb8-abfb-4b2245aea007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Dimensão TEMPO\n",
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "df_tempo = (\n",
    "    spark.table(\"mvp.gold.fact_expenditure_project_sdg_new\")\n",
    "    .select(col(\"year\").cast(\"int\").alias(\"Year\"))\n",
    "    .distinct()\n",
    "    .withColumn(\"SK_tempo\", expr(\"uuid()\"))\n",
    "    .select(\"SK_tempo\", \"Year\")\n",
    ")\n",
    "\n",
    "df_tempo.write.mode(\"overwrite\").saveAsTable(\"mvp.gold.dim_tempo_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "935e2a9e-e013-456c-8c74-db02dc8d5037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "ALTER TABLE gold.dim_tempo_new RENAME TO gold.dim_tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2ec547e-5dac-4465-b8ae-0f38bac99573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th></tr></thead><tbody><tr><td>72</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         72
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "num_affected_rows",
            "nullable": true,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 112
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "UPDATE mvp.silver.br_jwp_silver\n",
    "SET LNOB_groups_targeted = '-'\n",
    "WHERE LNOB_groups_targeted = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f59f5fba-d3b0-4862-9c28-2174baec827f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "DropTable"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DROP TABLE mvp.gold.dim_beneficiaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "680b03b0-a035-4b0d-bd6f-ef82a4b7f888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Dimensão BENEFICIARIES\n",
    "from pyspark.sql.functions import expr, col, explode, split, trim, when\n",
    "\n",
    "df = spark.table(\"mvp.silver.br_jwp_silver\").select(\n",
    "    \"ID\", \"LNOB_groups_targeted\",\n",
    ")\n",
    "\n",
    "df_beneficiaries = (\n",
    "    df.withColumn(\n",
    "        \"Beneficiaries_raw\",\n",
    "        trim(\n",
    "            expr(\"concat_ws(';', LNOB_groups_targeted)\")\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"Beneficiary\",\n",
    "        explode(\n",
    "            split(col(\"Beneficiaries_raw\"), \";\")\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"Beneficiary\",\n",
    "        trim(\n",
    "            when(\n",
    "                (col(\"Beneficiary\").rlike(\"(?i)other|outros|outro\")) | (col(\"Beneficiary\") == \"-\"),\n",
    "                \"-\"\n",
    "            ).when(\n",
    "                col(\"Beneficiary\").rlike(\"^[0-9]+$\"),\n",
    "                col(\"Beneficiary\")\n",
    "            ).otherwise(col(\"Beneficiary\"))\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"sk_beneficiary\",\n",
    "        expr(\"uuid()\")\n",
    "    )\n",
    "    .select(\n",
    "        col(\"sk_beneficiary\"),\n",
    "        col(\"ID\"),\n",
    "        col(\"Beneficiary\").alias(\"Beneficiaries\")\n",
    "    )\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "df_beneficiaries.write.mode(\"overwrite\").saveAsTable(\"mvp.gold.dim_beneficiaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10164720-80bf-4dca-bb1f-558029c0bb4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "AlterTable"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "ALTER TABLE mvp.gold.dim_beneficiaries \n",
    "ALTER COLUMN sk_beneficiary SET NOT NULL;\n",
    "\n",
    "ALTER TABLE mvp.gold.dim_beneficiaries \n",
    "ADD PRIMARY KEY (sk_beneficiary);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62e93b0f-7839-432f-9e5e-3778c415c984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Tabela dimensão TEMPO\n",
    "df_tempo = spark.table(\"mvp.gold.dim_tempo\")\n",
    "\n",
    "df_fact_final = (\n",
    "    df_fact_long\n",
    "    .join(df_tempo, on=\"Year\", how=\"left\")\n",
    "    .select(\n",
    "        col(\"ID\"),\n",
    "        col(\"sdg_sk\"),\n",
    "        col(\"sk_tempo\"),\n",
    "        col(\"Expenditure\").cast(\"decimal(18,2)\").alias(\"Expenditure\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "207e0abf-54e8-4d18-bc5d-e75817056855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Tabela Fato GASTO X ODS X ANO\n",
    "from pyspark.sql.functions import expr, col, ltrim, split, explode, size, when\n",
    "\n",
    "df_silver = spark.table(\"mvp.silver.br_jwp_silver\").select(\"ID\", \"SDG_Goals\", \"2023_Expenditure\", \"2024_Expenditure\", \"2025_Expenditure\")\n",
    "df_sdg = spark.table(\"mvp.gold.dim_sdg\").select(\"sdg_sk\", \"ID\", \"SDG\")\n",
    "\n",
    "# Explode SDGs and count how many per row\n",
    "df_exploded = (\n",
    "    df_silver\n",
    "    .withColumn(\"SDG_list\", split(col(\"SDG_Goals\"), \";\"))\n",
    "    .withColumn(\"num_sdgs\", size(col(\"SDG_list\")))\n",
    "    .withColumn(\"SDG\", explode(col(\"SDG_list\")))\n",
    "    .withColumn(\"SDG\", ltrim(col(\"SDG\")))\n",
    ")\n",
    "\n",
    "# Distribute expenditures equally across SDGs\n",
    "df_distributed = (\n",
    "    df_exploded\n",
    "    .withColumn(\"2023_Expenditure\", (col(\"2023_Expenditure\") / col(\"num_sdgs\")).cast(\"decimal(18,2)\"))\n",
    "    .withColumn(\"2024_Expenditure\", (col(\"2024_Expenditure\") / col(\"num_sdgs\")).cast(\"decimal(18,2)\"))\n",
    "    .withColumn(\"2025_Expenditure\", (col(\"2025_Expenditure\") / col(\"num_sdgs\")).cast(\"decimal(18,2)\"))\n",
    ")\n",
    "\n",
    "df_joined = (\n",
    "    df_distributed\n",
    "    .join(df_sdg, on=[\"ID\", \"SDG\"], how=\"left\")\n",
    "    .select(\n",
    "        col(\"ID\"),\n",
    "        col(\"sdg_sk\"),\n",
    "        col(\"2023_Expenditure\"),\n",
    "        col(\"2024_Expenditure\"),\n",
    "        col(\"2025_Expenditure\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_joined.write.mode(\"overwrite\").saveAsTable(\"mvp.gold.fact_expenditure_sdg_year_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "675ccd8c-0380-4e06-ba8b-95ba85c36de3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Criação da tabela-fato GASTO X PARCEIROS X ANO\n",
    "from pyspark.sql.functions import split, explode, size, col, ltrim, expr\n",
    "\n",
    "# Load base tables\n",
    "df_silver = spark.table(\"mvp.silver.br_jwp_silver\").select(\n",
    "    \"ID\", \"Contributing_partners\", \"Agencies\", \"2023_Expenditure\", \"2024_Expenditure\", \"2025_Expenditure\"\n",
    ")\n",
    "df_partner = spark.table(\"mvp.gold.dim_contributing_partners\").select(\"sk_partner\", \"ID\", \"Partner\")\n",
    "df_agency = spark.table(\"mvp.gold.dim_agency\").select(\"sk_agency\", \"Agency\")\n",
    "df_tempo = spark.table(\"mvp.gold.dim_tempo\").select(\"SK_tempo\", \"Year\")\n",
    "\n",
    "# Explode partners and count per row\n",
    "df_exploded = (\n",
    "    df_silver\n",
    "    .withColumn(\"Partner_list\", split(col(\"Contributing_partners\"), \";\"))\n",
    "    .withColumn(\"num_partners\", size(col(\"Partner_list\")))\n",
    "    .withColumn(\"Partner\", explode(col(\"Partner_list\")))\n",
    "    .withColumn(\"Partner\", ltrim(col(\"Partner\")))\n",
    ")\n",
    "\n",
    "# Distribute expenditures equally across partners\n",
    "df_distributed = (\n",
    "    df_exploded\n",
    "    .withColumn(\"2023_Expenditure\", (col(\"2023_Expenditure\") / col(\"num_partners\")).cast(\"decimal(18,2)\"))\n",
    "    .withColumn(\"2024_Expenditure\", (col(\"2024_Expenditure\") / col(\"num_partners\")).cast(\"decimal(18,2)\"))\n",
    "    .withColumn(\"2025_Expenditure\", (col(\"2025_Expenditure\") / col(\"num_partners\")).cast(\"decimal(18,2)\"))\n",
    ")\n",
    "\n",
    "# Join with partner SK\n",
    "df_joined_partner = (\n",
    "    df_distributed\n",
    "    .join(df_partner, on=[\"ID\", \"Partner\"], how=\"left\")\n",
    ")\n",
    "\n",
    "# Join with agency SK\n",
    "df_joined_agency = (\n",
    "    df_joined_partner\n",
    "    .join(df_agency, df_joined_partner.Agencies == df_agency.Agency, how=\"left\")\n",
    ")\n",
    "\n",
    "# Unpivot expenditures by year for tempo join\n",
    "df_long = (\n",
    "    df_joined_agency\n",
    "    .select(\n",
    "        \"ID\", \"sk_partner\", \"sk_agency\",\n",
    "        expr(\"2023_Expenditure as Expenditure\"), expr(\"2023 as Year\")\n",
    "    )\n",
    "    .unionByName(\n",
    "        df_joined_agency.select(\n",
    "            \"ID\", \"sk_partner\", \"sk_agency\",\n",
    "            expr(\"2024_Expenditure as Expenditure\"), expr(\"2024 as Year\")\n",
    "        )\n",
    "    )\n",
    "    .unionByName(\n",
    "        df_joined_agency.select(\n",
    "            \"ID\", \"sk_partner\", \"sk_agency\",\n",
    "            expr(\"2025_Expenditure as Expenditure\"), expr(\"2025 as Year\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Join with tempo SK\n",
    "df_final = (\n",
    "    df_long\n",
    "    .join(df_tempo, df_long.Year == df_tempo.Year, how=\"left\")\n",
    "    .select(\n",
    "        \"ID\", \"sk_partner\", \"sk_agency\", \"SK_tempo\", \"Expenditure\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df_final.write.mode(\"overwrite\").saveAsTable(\"mvp.gold.fact_expenditure_partner_year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c103deb0-1f3f-4363-b3a3-ab5047cace41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, size, col, ltrim, expr, trim\n",
    "\n",
    "# Load base tables\n",
    "df_silver = spark.table(\"mvp.silver.br_jwp_silver\").select(\n",
    "    \"ID\", \"Agencies\", \"LNOB_groups_targeted\", \"2023_Expenditure\", \"2024_Expenditure\", \"2025_Expenditure\"\n",
    ")\n",
    "df_project = spark.table(\"mvp.gold.dim_projeto\").select(\"sk_project\", \"ID_Projeto\")\n",
    "df_agency = spark.table(\"mvp.gold.dim_agency\").select(\"sk_agency\", \"Agency\")\n",
    "df_tempo = spark.table(\"mvp.gold.dim_tempo\").select(\"SK_tempo\", \"Year\")\n",
    "df_beneficiary = spark.table(\"mvp.gold.dim_beneficiaries\").select(\"sk_beneficiary\", \"ID\", \"Beneficiaries\")\n",
    "\n",
    "# Explode beneficiaries and count per row\n",
    "df_exploded = (\n",
    "    df_silver\n",
    "    .withColumn(\"Beneficiary_list\", split(trim(col(\"LNOB_groups_targeted\")), \";\"))\n",
    "    .withColumn(\"num_beneficiaries\", size(col(\"Beneficiary_list\")))\n",
    "    .withColumn(\"Beneficiaries\", explode(col(\"Beneficiary_list\")))\n",
    "    .withColumn(\"Beneficiaries\", ltrim(trim(col(\"Beneficiaries\"))))\n",
    ")\n",
    "\n",
    "# Distribute expenditures equally across beneficiaries\n",
    "df_distributed = (\n",
    "    df_exploded\n",
    "    .withColumn(\"2023_Expenditure\", (col(\"2023_Expenditure\") / col(\"num_beneficiaries\")).cast(\"decimal(18,2)\"))\n",
    "    .withColumn(\"2024_Expenditure\", (col(\"2024_Expenditure\") / col(\"num_beneficiaries\")).cast(\"decimal(18,2)\"))\n",
    "    .withColumn(\"2025_Expenditure\", (col(\"2025_Expenditure\") / col(\"num_beneficiaries\")).cast(\"decimal(18,2)\"))\n",
    ")\n",
    "\n",
    "# Join with beneficiary SK\n",
    "df_joined_beneficiary = (\n",
    "    df_distributed\n",
    "    .join(df_beneficiary, on=[\"ID\", \"Beneficiaries\"], how=\"left\")\n",
    ")\n",
    "\n",
    "# Join with project SK using correct table and column\n",
    "df_joined_project = (\n",
    "    df_joined_beneficiary\n",
    "    .join(df_project, df_joined_beneficiary.ID == df_project.ID_Projeto, how=\"left\")\n",
    ")\n",
    "\n",
    "# Join with agency SK\n",
    "df_joined_agency = (\n",
    "    df_joined_project\n",
    "    .join(df_agency, df_joined_project.Agencies == df_agency.Agency, how=\"left\")\n",
    ")\n",
    "\n",
    "# Unpivot expenditures by year for tempo join\n",
    "df_long = (\n",
    "    df_joined_agency\n",
    "    .select(\n",
    "        \"ID\", \"sk_project\", \"sk_agency\", \"sk_beneficiary\",\n",
    "        expr(\"2023_Expenditure as Expenditure\"), expr(\"2023 as Year\")\n",
    "    )\n",
    "    .unionByName(\n",
    "        df_joined_agency.select(\n",
    "            \"ID\", \"sk_project\", \"sk_agency\", \"sk_beneficiary\",\n",
    "            expr(\"2024_Expenditure as Expenditure\"), expr(\"2024 as Year\")\n",
    "        )\n",
    "    )\n",
    "    .unionByName(\n",
    "        df_joined_agency.select(\n",
    "            \"ID\", \"sk_project\", \"sk_agency\", \"sk_beneficiary\",\n",
    "            expr(\"2025_Expenditure as Expenditure\"), expr(\"2025 as Year\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Join with tempo SK\n",
    "df_final = (\n",
    "    df_long\n",
    "    .join(df_tempo, df_long.Year == df_tempo.Year, how=\"left\")\n",
    "    .select(\n",
    "        \"ID\", \"sk_project\", \"sk_agency\", \"SK_tempo\", \"sk_beneficiary\", \"Expenditure\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df_final.write.mode(\"overwrite\").saveAsTable(\"mvp.gold.expediture_beneficiaries_year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57fac015-c1eb-4f62-b6be-1fdd64d2b74e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "AlterTable"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "ALTER TABLE mvp.gold.expediture_beneficiaries_year rename to mvp.gold.fact_expediture_beneficiaries_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "440f020a-dcba-4aea-9dd0-cf28c0fadb6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_fact_final.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"mvp.gold.fact_expenditure_sdg_year_normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7738b9dd-9e7d-4af7-96d7-be900ce0a5b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Feitas todas estas etapas, rodarei alguns testes para garantir que os valores de expenditure foram devidamente distribuídos, seguindo fielmente os dados na tabela silver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcf78c95-91ea-4594-b95c-2c6525d10f51",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766003775684}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  ID,\n",
    "  SDG_Goals,\n",
    "  2023_Expenditure,\n",
    "  2024_Expenditure,\n",
    "  2025_Expenditure\n",
    "FROM mvp.silver.br_jwp_silver\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d74dfc-b367-4643-ba75-24f031123c48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Neste primeiro teste na relação EXPENDITURE X SDG YEAR, temos que:\n",
    "- Projeto de ID 134839 teve uma execução de USD 532.569 em 2023. Como o projeto está associado a 4 ODSs, espera-se um resultado explodido deste ID em 4 linhas, no valor de USD 133.142,25 por ODS.\n",
    "- Projeto de ID 179569 teve uma execução de USD 22.427 em 2024. Como o projeto está associado a apenas 1 ODS, espera-se um resultado normal.\n",
    "- Projeto de ID 200514 teve uma execução de USD 12.813 em 2025. Como o projeto está associado a 3 ODSs, espera-se um resultado explodido deste ID em 3 linhas, no valor de USD 4.271 por ODS.\n",
    "- Projeto de ID 200515 teve uma execução de USD 14.212 em 2025. Como o projeto está associado a 3 ODSs, espera-se um resultado explodido deste ID em 3 linhas, no valor de USD 4.737,33 por ODS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f29868c6-66e4-434d-bcac-6aab7957f4f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  ID,\n",
    "  sk_tempo,\n",
    "  Expenditure\n",
    "FROM mvp.gold.fact_expenditure_sdg_year_normalized\n",
    "WHERE ID IN ('134839', '179569', '200514', '200515')\n",
    "  AND Expenditure <> 0.00\n",
    "ORDER BY ID, sk_tempo;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cdcf428-6aef-43d7-b814-f30a77c550d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Neste segundo teste na relação EXPENDITURE X PARTNER x YEAR, temos que:\n",
    "- Projeto de ID 134813 teve uma execução de USD 364.236 em 2023. Como o projeto está associado a 3 parceiros, espera-se um resultado explodido deste ID em 4 linhas, no valor de USD 121.412 por parceiro.\n",
    "- Projeto de ID 134832 teve uma execução de USD 171.279 em 2024. Como o projeto está associado a apenas 1 parceiro, espera-se um resultado normal.\n",
    "- Projeto de ID 134816 teve uma execução de USD 298.732 em 2025. Como o projeto está associado a 5 parceiros, espera-se um resultado explodido deste ID em 3 linhas, no valor de USD 59.746,40 por pareiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4214b8c-0fc4-4ee5-be56-51cc62259ed7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766085295797}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sk_partner</th><th>Partner</th><th>Expenditure</th></tr></thead><tbody><tr><td>8a01df64-7e7a-4aa6-967d-3e313fd7fb9f</td><td>Luxembourg</td><td>121412.00</td></tr><tr><td>2dfbbaad-af07-4b4d-a6a8-fabdd1587ce2</td><td>United Nations Population Fund</td><td>121412.00</td></tr><tr><td>1470d9ac-017f-4775-95ee-b7382b49dd6e</td><td>United States Agency for International Development</td><td>121412.00</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "8a01df64-7e7a-4aa6-967d-3e313fd7fb9f",
         "Luxembourg",
         "121412.00"
        ],
        [
         "2dfbbaad-af07-4b4d-a6a8-fabdd1587ce2",
         "United Nations Population Fund",
         "121412.00"
        ],
        [
         "1470d9ac-017f-4775-95ee-b7382b49dd6e",
         "United States Agency for International Development",
         "121412.00"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "sk_partner",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "Partner",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "Expenditure",
            "nullable": true,
            "type": "decimal(18,2)"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 248
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "sk_partner",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Partner",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Expenditure",
         "type": "\"decimal(18,2)\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    f.sk_partner, \n",
    "    cp.Partner,\n",
    "    f.Expenditure\n",
    "FROM \n",
    "    mvp.gold.fact_expenditure_partner_year f\n",
    "JOIN \n",
    "    mvp.gold.dim_tempo t\n",
    "    ON f.SK_tempo = t.sk_tempo\n",
    "JOIN \n",
    "    mvp.gold.dim_contributing_partners cp\n",
    "    ON f.sk_partner = cp.sk_partner\n",
    "WHERE \n",
    "    f.ID = '134813' \n",
    "    AND t.Year = 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3378b138-3c74-4283-9d26-b40f8655d31c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sk_partner</th><th>Partner</th><th>Expenditure</th></tr></thead><tbody><tr><td>7cc58f60-5d61-4d02-9ff4-ff667c8d1266</td><td>ABC - Brazilian Cooperation Agency</td><td>171279.00</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "7cc58f60-5d61-4d02-9ff4-ff667c8d1266",
         "ABC - Brazilian Cooperation Agency",
         "171279.00"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "sk_partner",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "Partner",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "Expenditure",
            "nullable": true,
            "type": "decimal(18,2)"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 251
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "sk_partner",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Partner",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Expenditure",
         "type": "\"decimal(18,2)\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    f.sk_partner, \n",
    "    cp.Partner,\n",
    "    f.Expenditure\n",
    "FROM \n",
    "    mvp.gold.fact_expenditure_partner_year f\n",
    "JOIN \n",
    "    mvp.gold.dim_tempo t\n",
    "    ON f.SK_tempo = t.sk_tempo\n",
    "JOIN \n",
    "    mvp.gold.dim_contributing_partners cp\n",
    "    ON f.sk_partner = cp.sk_partner\n",
    "WHERE \n",
    "    f.ID = '134832' \n",
    "    AND t.Year = 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57d9ed1b-f1fa-4c06-990c-b362d6dd6721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sk_partner</th><th>Partner</th><th>Expenditure</th></tr></thead><tbody><tr><td>16099323-7464-4b89-83dd-3d2d4ebfa4d5</td><td>#BR UNHCR PSP Brazil</td><td>59746.40</td></tr><tr><td>716238c9-b2f7-41b1-9510-07aeaf99f260</td><td>Inmigration, Refugees and Citizenship Canada</td><td>59746.40</td></tr><tr><td>baf5eff8-806d-460a-86b6-db18456aa621</td><td>Luxembourg</td><td>59746.40</td></tr><tr><td>35dcc980-8391-4ede-970a-5276d34d6724</td><td>The US Government Department of State's Bureau of Population, Refugees and Migration</td><td>59746.40</td></tr><tr><td>9c9746d0-27a5-4355-b67d-f97008adec27</td><td>United Nations High Commissioner for Refugees</td><td>59746.40</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "16099323-7464-4b89-83dd-3d2d4ebfa4d5",
         "#BR UNHCR PSP Brazil",
         "59746.40"
        ],
        [
         "716238c9-b2f7-41b1-9510-07aeaf99f260",
         "Inmigration, Refugees and Citizenship Canada",
         "59746.40"
        ],
        [
         "baf5eff8-806d-460a-86b6-db18456aa621",
         "Luxembourg",
         "59746.40"
        ],
        [
         "35dcc980-8391-4ede-970a-5276d34d6724",
         "The US Government Department of State's Bureau of Population, Refugees and Migration",
         "59746.40"
        ],
        [
         "9c9746d0-27a5-4355-b67d-f97008adec27",
         "United Nations High Commissioner for Refugees",
         "59746.40"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "sk_partner",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "Partner",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "Expenditure",
            "nullable": true,
            "type": "decimal(18,2)"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 253
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "sk_partner",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Partner",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Expenditure",
         "type": "\"decimal(18,2)\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    f.sk_partner, \n",
    "    cp.Partner,\n",
    "    f.Expenditure\n",
    "FROM \n",
    "    mvp.gold.fact_expenditure_partner_year f\n",
    "JOIN \n",
    "    mvp.gold.dim_tempo t\n",
    "    ON f.SK_tempo = t.sk_tempo\n",
    "JOIN \n",
    "    mvp.gold.dim_contributing_partners cp\n",
    "    ON f.sk_partner = cp.sk_partner\n",
    "WHERE \n",
    "    f.ID = '134816' \n",
    "    AND t.Year = 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e11321d-4f39-48b3-a029-35816b6305d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Teste realizado com sucesso!\n",
    "Como podemos ver, os valores estão devidamente distribuídos por ODS e por ano. Este era o maior desafio encontrado na vida real para realizar análises em cima da versão beta desta base de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5c83b8e-afd1-4293-920c-3ce437958632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 7. Análise dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941d98f5-ec62-40a6-a1a9-5c948ff80598",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Nesta última etapa do MVP, eu pretendo responder a algumas das perguntas levantadas na seção (2). Para isso, usarei a função DASHBOARD do Databricks para gerar gráficos ilustrativos. Para facilitar o uso da ferramenta, criarei um VIEW para que o dashboard possa relacionar a tabela fato mvp.gold.fact_expenditure_sdg_year_normalized com suas dimensões mvp.gold.dim_sdg e mvp.gold.dim_tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f763d68-43c2-4428-91c8-5cb63e26b4f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Definir o uso padrão do ESQUEMA OURO\n",
    "spark.sql(\"USE CATALOG mvp\")\n",
    "spark.sql(\"USE SCHEMA gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3218e477-0cc9-4615-a43c-8575034002de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW mvp.gold.vw_expenditure_sdg_year_normalized AS\n",
    "SELECT\n",
    "    f.ID,\n",
    "    d.SDG,\n",
    "    t.Year,\n",
    "    f.Expenditure\n",
    "FROM mvp.gold.fact_expenditure_sdg_year_normalized f\n",
    "JOIN mvp.gold.dim_sdg d\n",
    "  ON f.sdg_sk = d.sdg_sk\n",
    "JOIN mvp.gold.dim_tempo t\n",
    "  ON f.sk_tempo = t.sk_tempo;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2132f8df-d236-47a4-a9ae-bbe46a02ea03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "CreateView"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW mvp.gold.vw_expenditure_partner_year AS\n",
    "SELECT\n",
    "    f.ID,\n",
    "    f.sk_partner,\n",
    "    f.SK_tempo,\n",
    "    f.Expenditure,\n",
    "    a.sk_agency,\n",
    "    t.Year\n",
    "FROM mvp.gold.fact_expenditure_partner_year f\n",
    "JOIN mvp.gold.dim_projeto p\n",
    "  ON f.ID = p.ID_Projeto\n",
    "JOIN mvp.gold.dim_agency a\n",
    "  ON f.sk_agency = a.sk_agency\n",
    "JOIN mvp.gold.dim_tempo t\n",
    "  ON f.SK_tempo = t.sk_tempo\n",
    "JOIN mvp.gold.dim_contributing_partners cp\n",
    "  ON f.sk_partner = cp.sk_partner;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21d36f93-c074-454a-80c1-0fc2defbdbd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Primeira pergunta analítica\n",
    "Na somatória de recursos executados de 2023 a 2025, o financiamento recebido pelas agências, fundos e programas da ONU no Brasil tem sido direcionado a quais ODS? Qual ODS recebeu mais recursos até o momento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a13a3248-1abe-4dda-9411-058005467601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  d.SDG,\n",
    "  SUM(CASE WHEN t.Year = 2023 THEN f.Expenditure ELSE 0 END) AS Expenditure_2023,\n",
    "  SUM(CASE WHEN t.Year = 2024 THEN f.Expenditure ELSE 0 END) AS Expenditure_2024,\n",
    "  SUM(CASE WHEN t.Year = 2025 THEN f.Expenditure ELSE 0 END) AS Expenditure_2025,\n",
    "  SUM(f.Expenditure) AS Total_Expenditure\n",
    "FROM mvp.gold.fact_expenditure_sdg_year_normalized f\n",
    "JOIN mvp.gold.dim_sdg d\n",
    "  ON f.sdg_sk = d.sdg_sk\n",
    "JOIN mvp.gold.dim_tempo t\n",
    "  ON f.sk_tempo = t.sk_tempo\n",
    "GROUP BY d.SDG\n",
    "ORDER BY Total_Expenditure DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1be1920-3969-410f-9b8d-b117bfb14f65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O query acima indica que o **ODS 4 \"Quality Education\"** t**eve o maior volume de recursos executados entre 2023 e 2025, totalizando USD 44.943.670, 49**. A liderança foi seguida pelos ods 17 \" Partnership for the Goals\" (USD 30.348.492,82) e ODS 16 \"Peace and Justice - Strong Institutions\" ( (USD) 25.407.434,81).\n",
    "\n",
    "Já o ODS 7, relacionado à disponibilização e garantia equitativa de energia limpa e acessível teve o menor volume de investimentos eecutados, somando UDS 583.096,25."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8871297968000360,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Relatório MVP Engenharia de Dados",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}